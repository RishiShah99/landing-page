<!DOCTYPE html>
<html lang="en" style="cursor: none !important; background-color: #020510 !important;">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Architecture-Driven Adversarial Robustness - Enhanced CORnet-S | Rishi Shah</title>
    <meta name="description" content="Novel machine learning research achieving 97.82% adversarial robustness through architectural innovation. Enhanced CORnet-S with learnable prefiltering, gated recurrence, and denoise-scaling with no adversarial training required.">
    <style>
        /* CRITICAL: Hide cursor and background immediately on page load */
        html, body, * {
            cursor: none !important;
        }
        html, body {
            background-color: #020510 !important;
            margin: 0;
            padding: 0;
        }
    </style>
    <link rel="prefetch" href="../index.html">
    <link rel="preload" href="../style.css" as="style">
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <script src="https://kit.fontawesome.com/17ea408dcc.js" crossorigin="anonymous"></script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        :root {
            --primary: #00b4d8;
            --secondary: #5EEAD4;
            --dark: #020510;
            --darker: #0a1628;
            --light: #e0f2fe;
            --accent: #38bdf8;
        }

        body {
            font-family: 'Poppins', sans-serif;
            background: linear-gradient(135deg, var(--dark) 0%, var(--darker) 100%);
            color: var(--light);
            line-height: 1.6;
            min-height: 100vh;
            overflow-x: hidden;
            cursor: none !important;
        }

        * {
            cursor: none !important;
        }

        #particles-js {
            position: fixed;
            width: 100%;
            height: 100%;
            top: 0;
            left: 0;
            z-index: 5;
            pointer-events: none;
        }

        .back-button {
            position: fixed;
            top: 2rem;
            left: 2rem;
            z-index: 10001;
            width: 48px;
            height: 48px;
            display: flex;
            align-items: center;
            justify-content: center;
            background: rgba(0, 180, 216, 0.1);
            border: 1px solid var(--primary);
            color: var(--primary);
            text-decoration: none;
            border-radius: 50%;
            font-size: 1.2rem;
            transition: all 0.4s cubic-bezier(0.4, 0, 0.2, 1);
            backdrop-filter: blur(10px);
            opacity: 1;
            transform: translateX(0);
        }

        .back-button.hidden {
            opacity: 0;
            transform: translateX(-20px);
            pointer-events: none;
        }

        .back-button:hover {
            background: rgba(0, 180, 216, 0.2);
            transform: scale(1.1);
            box-shadow: 0 0 30px rgba(0, 180, 216, 0.4);
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 6rem 2rem 4rem;
            position: relative;
            z-index: 10;
        }

        .project-header {
            text-align: center;
            margin-bottom: 4rem;
            padding: 3rem 2rem;
            background: rgba(10, 22, 40, 0.5);
            border-radius: 16px;
            border: 1px solid rgba(0, 180, 216, 0.2);
            position: relative;
            overflow: hidden;
        }

        .project-header::before {
            content: '';
            position: absolute;
            top: 0;
            left: -100%;
            width: 100%;
            height: 2px;
            background: linear-gradient(90deg, transparent, var(--primary), transparent);
            animation: scan 3s infinite;
        }

        @keyframes scan {
            0% { left: -100%; }
            100% { left: 100%; }
        }

        .project-header h1 {
            font-size: 3rem;
            font-weight: 700;
            margin-bottom: 1rem;
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        .project-subtitle {
            font-size: 1.1rem;
            color: var(--secondary);
            font-weight: 300;
            margin-bottom: 0.5rem;
        }

        .project-meta {
            display: flex;
            justify-content: center;
            gap: 2rem;
            margin-top: 1.5rem;
            font-size: 0.9rem;
            color: rgba(255, 255, 255, 0.6);
        }

        .section {
            margin-bottom: 2rem;
            padding: 2rem;
            background: rgba(10, 22, 40, 0.3);
            border-radius: 12px;
            border: 1px solid rgba(0, 180, 216, 0.1);
        }

        .section-header {
            font-size: 2rem;
            font-weight: 600;
            margin-bottom: 1.5rem;
            color: var(--primary);
            font-family: 'Courier New', monospace;
        }

        .highlight-box {
            background: rgba(0, 180, 216, 0.05);
            border-left: 4px solid var(--primary);
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-radius: 8px;
        }

        /* Document-style formatting */
        .document-section {
            margin-bottom: 2rem;
            padding: 3rem;
            background: rgba(10, 22, 40, 0.3);
            border-radius: 12px;
            border: 1px solid rgba(0, 180, 216, 0.1);
        }

        .document-section h1 {
            font-size: 2rem;
            font-weight: 600;
            color: var(--primary);
            font-family: 'Courier New', monospace;
            margin-bottom: 1.5rem;
            margin-top: 2.5rem;
        }

        .document-section h1:first-child {
            margin-top: 0;
        }

        .document-section h2 {
            font-size: 1.3rem;
            font-weight: 500;
            color: var(--secondary);
            margin-bottom: 1rem;
            margin-top: 2rem;
        }

        .document-section p {
            font-size: 1rem;
            line-height: 1.8;
            color: rgba(255, 255, 255, 0.85);
            margin-bottom: 1rem;
        }

        .document-section ul {
            margin: 1rem 0 1rem 1.5rem;
            list-style-type: disc;
        }

        .document-section ul li {
            font-size: 1rem;
            line-height: 1.8;
            color: rgba(255, 255, 255, 0.85);
            margin-bottom: 0.5rem;
        }

        .document-section strong {
            color: #ffffff;
            font-weight: 600;
        }

        .stats-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 1.5rem;
            margin: 2rem 0;
        }

        .stat-card {
            background: rgba(0, 180, 216, 0.05);
            padding: 1.5rem;
            border-radius: 12px;
            border: 1px solid rgba(0, 180, 216, 0.2);
            text-align: center;
            transition: all 0.3s ease;
        }

        .stat-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 30px rgba(0, 180, 216, 0.2);
            border-color: var(--primary);
        }

        .stat-number {
            font-size: 2.5rem;
            font-weight: 700;
            color: var(--primary);
            margin-bottom: 0.5rem;
        }

        .stat-label {
            font-size: 0.9rem;
            color: rgba(255, 255, 255, 0.7);
        }

        .tech-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 1rem;
            margin: 2rem 0;
        }

        .tech-item {
            background: rgba(0, 180, 216, 0.05);
            padding: 1rem;
            border-radius: 8px;
            border: 1px solid rgba(0, 180, 216, 0.2);
            text-align: center;
            font-weight: 500;
            transition: all 0.3s ease;
        }

        .tech-item:hover {
            background: rgba(0, 180, 216, 0.1);
            border-color: var(--primary);
            transform: scale(1.05);
        }

        @media (max-width: 768px) {
            .project-header h1 {
                font-size: 2rem;
            }

            .container {
                padding: 5rem 1rem 2rem;
            }

            .back-button {
                top: 1rem;
                left: 1rem;
                padding: 0.5rem 1rem;
            }

            .stats-grid {
                grid-template-columns: 1fr;
            }
        }

        .project-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            margin-top: 1rem;
            padding: 0.75rem 1.5rem;
            background: rgba(0, 180, 216, 0.1);
            border: 1px solid var(--primary);
            color: var(--primary);
            text-decoration: none;
            border-radius: 8px;
            transition: all 0.3s ease;
        }

        .project-link:hover {
            background: rgba(0, 180, 216, 0.2);
            box-shadow: 0 0 20px rgba(0, 180, 216, 0.3);
            transform: translateY(-2px);
        }

        /* Custom Cursor */
        .cursor {
            width: 16px;
            height: 16px;
            border: 2px solid #00b4d8;
            border-radius: 50%;
            position: fixed;
            pointer-events: none;
            z-index: 9999;
            transition: transform 0.05s ease-out, opacity 0.2s ease, width 0.2s ease, height 0.2s ease;
            transform: translate(-50%, -50%);
            box-shadow: 0 0 10px rgba(0, 180, 220, 0.5);
            opacity: 1;
            will-change: transform;
        }

        .cursor.hover {
            width: 32px;
            height: 32px;
            background: rgba(0, 180, 220, 0.1);
            border-width: 3px;
            box-shadow: 0 0 20px rgba(0, 180, 220, 0.8);
        }

        .cursor-glow {
            width: 40px;
            height: 40px;
            background: radial-gradient(circle, rgba(0, 180, 220, 0.2) 0%, transparent 70%);
            border-radius: 50%;
            position: fixed;
            pointer-events: none;
            z-index: 9998;
            transition: transform 0.1s ease-out, width 0.2s ease, height 0.2s ease;
            transform: translate(-50%, -50%);
            will-change: transform;
        }

        .cursor-glow.hover {
            width: 60px;
            height: 60px;
            background: radial-gradient(circle, rgba(0, 180, 220, 0.3) 0%, transparent 70%);
        }

        a, button, .stat-card, .tech-item {
            cursor: none !important;
        }
    </style>
</head>
<body>
    <div id="particles-js"></div>
    
    <a href="../index.html" class="back-button">
        <i class="fas fa-arrow-left"></i>
    </a>

    <div class="container">
        <!-- Box 1: Header -->
        <div class="project-header">
            <div class="project-subtitle">MACHINE LEARNING RESEARCH</div>
            <h1>Architecture-Driven Adversarial Robustness</h1>
            <div class="project-subtitle">Enhanced CORnet-S with Biologically-Inspired Defense Mechanisms</div>
            <div class="project-meta">
                <span>âš¡ 97.82% Robustness</span>
                <span>ðŸ§  Biologically-Inspired</span>
                <span>ðŸŽ¯ No Adversarial Training</span>
            </div>
            <a href="https://github.com/RishiShah99" target="_blank" class="project-link">
                <i class="fab fa-github"></i> View on GitHub
            </a>
        </div>

        <!-- Box 2: Overview -->
        <section class="section">
            <h2 class="section-header">Overview</h2>
            <div class="highlight-box">
                <p><strong>Architecture-Driven Adversarial Robustness</strong> achieves 97.82% robustness against sophisticated adversarial attacks through architectural innovation aloneâ€”no adversarial training required. By enhancing CORnet-S, a biologically-inspired neural network, with learnable prefiltering, gated recurrent processing, and adaptive noise suppression, this research demonstrates that intelligent design can replace computationally expensive training paradigms.</p>
            </div>
            <p>Adversarial attacks pose a critical threat to deep learning systems, particularly in safety-critical applications like autonomous vehicles and medical diagnostics. Traditional defense methods rely on adversarial training, which is computationally expensive (10-100x longer training) and often degrades clean accuracy. This research takes a fundamentally different approach: rather than training models to resist attacks, it builds resistance directly into the architecture.</p>
            
            <p style="margin-top: 1rem;">Inspired by biological visual systems' natural resilience to noise, the enhanced CORnet-S incorporates retinal preprocessing (learnable prefilter block), cortical feedback loops (gated recurrent blocks), and adaptive neural suppression (denoise-scaling mechanism). Testing across MNIST, CIFAR-100, and ImageNet100 datasets against FGSM, PGD, CW, and Patch attacks revealed a consistent +10% improvement over baseline CORnet-S while maintaining competitive clean accuracy.</p>
        </section>

        <!-- Box 3: Technology Stack -->
        <section class="section">
            <h2 class="section-header">Technology Stack</h2>
            <div class="tech-grid">
                <div class="tech-item">PyTorch</div>
                <div class="tech-item">TorchAttacks</div>
                <div class="tech-item">NVIDIA CUDA</div>
                <div class="tech-item">NumPy</div>
                <div class="tech-item">Pandas</div>
                <div class="tech-item">Matplotlib</div>
                <div class="tech-item">Scikit-learn</div>
                <div class="tech-item">Kaggle GPU</div>
            </div>
        </section>

        <!-- Box 4: Main Document Content -->
        <section class="document-section">
            <h1>Research Motivation</h1>

            <h2>The Adversarial Vulnerability Problem</h2>
            <p>Artificial Neural Networks (ANNs) have become integral to modern technological systems, particularly in safety-critical applications like healthcare diagnostics, financial fraud detection, and autonomous vehicles. Despite their impressive performance, these networks are susceptible to adversarial attacksâ€”carefully crafted perturbations that fool models while remaining imperceptible to humans.</p>

            <p>Adversarial attacks exploit deep learning models' sensitivity to input perturbations. Fast Gradient Sign Method (FGSM) introduces small perturbations causing significant misclassification. More sophisticated attacks like Projected Gradient Descent (PGD) and Carlini-Wagner (CW) iteratively refine perturbations to maximize impact. Physical attacks, like Patch Attacks, modify specific input regions, posing real-world threats to autonomous vehicles and facial recognition systems.</p>

            <h2>Limitations of Existing Defenses</h2>
            <p>The standard defense approach is adversarial training, which augments training data with corrupted examples. While effective, this method presents several critical challenges:</p>
            <ul>
                <li><strong>Computational Expense:</strong> Adversarial training requires 10-100x longer training time compared to standard training, making it prohibitively expensive for large-scale deployments</li>
                <li><strong>Data Requirements:</strong> Massive data augmentation is needed to cover diverse attack scenarios, requiring significant storage and processing resources</li>
                <li><strong>Accuracy Degradation:</strong> Models trained adversarially often exhibit reduced clean accuracy, trading standard performance for robustness</li>
                <li><strong>Attack-Specific Defense:</strong> Training on specific attack types may not generalize to novel attack methods, requiring continuous retraining</li>
            </ul>

            <p>These limitations prompted a fundamental question: could intelligent architecture design replace expensive training paradigms?</p>

            <h2>Biological Inspiration for Robustness</h2>
            <p>Biological visual systems exhibit remarkable resilience to noise and perturbations that artificial neural networks struggle with. Three key principles from neuroscience guided this research:</p>
            <ul>
                <li><strong>Retinal Preprocessing:</strong> The human retina filters visual noise before cortical processing, removing high-frequency artifacts that could confuse higher-level processing</li>
                <li><strong>Cortical Feedback Loops:</strong> Visual cortex employs recurrent connections allowing "double-checking" of ambiguous stimuli, enabling iterative refinement of perceptual hypotheses</li>
                <li><strong>Neural Suppression:</strong> Biological neurons automatically suppress noisy signals through adaptive gain control and normalization mechanisms</li>
            </ul>

            <p>CORnet-S was chosen as the foundation because it already incorporates biologically-plausible design: four hierarchical stages (V1, V2, V4, IT) mirroring primate visual cortex and recurrent processing within each stage. However, baseline CORnet-S still exhibited vulnerabilities to adversarial attacks, suggesting that biological inspiration alone was insufficientâ€”targeted enhancements were needed.</p>

            <h1>Methodology</h1>

            <h2>Experimental Design</h2>
            <p>To rigorously evaluate adversarial robustness, experiments were conducted across three dimensions: datasets, attack methods, and model architectures.</p>

            <p><strong>Datasets:</strong></p>
            <ul>
                <li><strong>ImageNet100:</strong> Subset of ImageNet with 100 randomly selected classes, providing high-dimensional and diverse visual data challenging for both clean and adversarial performance</li>
                <li><strong>CIFAR-100:</strong> 100 classes with 600 images each (500 training, 100 test per class), offering moderate complexity with 32Ã—32 resolution for computational efficiency</li>
                <li><strong>MNIST:</strong> Handwritten digit classification with 10 classes, serving as a baseline for understanding adversarial vulnerabilities in low-dimensional image data</li>
            </ul>

            <p><strong>Adversarial Attack Methods:</strong></p>
            <ul>
                <li><strong>Fast Gradient Sign Method (FGSM):</strong> Single-step gradient-based attack perturbing images in the direction of the loss gradient, representing the simplest and fastest attack type</li>
                <li><strong>Projected Gradient Descent (PGD):</strong> Multi-step iterative attack refining perturbations through repeated gradient updates within epsilon-bounded space, considered one of the strongest white-box attacks</li>
                <li><strong>Carlini-Wagner (CW):</strong> Targeted optimization-based attack minimizing perturbation magnitude while maximizing misclassification confidence, extremely effective but computationally intensive</li>
                <li><strong>Patch Attack:</strong> Localized attack inserting adversarial patches into specific image regions, relevant for real-world physical attacks on autonomous systems</li>
            </ul>

            <p><strong>Baseline Architectures:</strong></p>
            <ul>
                <li><strong>CORnet-S:</strong> Neuroanatomically-aligned model with four hierarchical stages and recurrent processing, serving as the foundation for enhancements</li>
                <li><strong>ResNet18:</strong> Traditional CNN with residual connections, representing modern feedforward architectures with skip connections</li>
                <li><strong>AlexNet:</strong> Classic CNN architecture establishing foundational principles, serving as a simple baseline for comparison</li>
            </ul>

            <h2>Implementation Details</h2>
            <p>All models were implemented in PyTorch and trained on NVIDIA GPU infrastructure. The training pipeline consisted of:</p>
            <ul>
                <li><strong>Model Training:</strong> Each architecture trained from scratch on each dataset using standard preprocessing, normalization (mean subtraction, standard deviation division), and data augmentation (random cropping, horizontal flipping for CIFAR-100 and ImageNet100)</li>
                <li><strong>Optimization:</strong> Adam optimizer with initial learning rate of 0.001 and step decay scheduler reducing learning rate by factor of 0.1 every 30 epochs</li>
                <li><strong>Attack Evaluation:</strong> TorchAttacks library implementing FGSM, PGD, and CW attacks with varying epsilon values (0.01, 0.05, 0.1, 0.3); custom Patch Attack implementation simulating real-world adversarial patches</li>
                <li><strong>Performance Metrics:</strong> Top-1 and Top-5 accuracy recorded for both clean images and adversarially perturbed images; robustness measured as accuracy retention under attack</li>
            </ul>

            <h1>Architectural Innovations</h1>

            <h2>1. Learnable Prefilter Block</h2>
            <p>A lightweight learnable denoising module was placed before CORnet-S's feedforward path, functioning as a biological retinal preprocessing stage. This prefilter consists of small convolutional filters combined with nonlinear gating that cleans input images before they enter the main model architecture.</p>

            <p><strong>Biological Motivation:</strong> Inspired by retinal preprocessing in biological vision systems, where ganglion cells and bipolar cells filter visual information before it reaches the visual cortex. The retina naturally suppresses high-frequency noise and enhances relevant visual featuresâ€”a mechanism adversarial attacks exploit by injecting imperceptible high-frequency perturbations.</p>

            <p><strong>Technical Implementation:</strong> The prefilter operates on the principle that adversarial perturbations predominantly occupy high-frequency spatial domains, while semantic content resides in lower frequencies. By applying learned convolution filters with adaptive thresholding, the module selectively attenuates high-frequency components while preserving task-relevant features.</p>

            <ul>
                <li><strong>Adaptive Filtering:</strong> Unlike fixed Gaussian blur or median filters, the learnable prefilter adapts its frequency response during training to optimize both clean accuracy and robustness</li>
                <li><strong>Minimal Overhead:</strong> The lightweight architecture adds negligible computational cost (approximately 2% increase in forward pass time)</li>
                <li><strong>Gradient Obfuscation Prevention:</strong> The differentiable design prevents gradient masking, ensuring genuine robustness rather than artificial defense that obscures gradients</li>
            </ul>

            <p><strong>Impact:</strong> This preprocessing stage provided dramatic improvements against PGD and CW attacks, which heavily rely on high-frequency noise injection. By removing adversarial perturbations before the model processes them, the prefilter forces attackers to craft perturbations in the semantic feature space, which is a significantly harder problem.</p>

            <h2>2. Gated Recurrent Blocks</h2>
            <p>Standard CORnet-S residual connections were replaced with gated recurrent units that iterate 2-4 times per forward pass. Each recurrent block incorporates sigmoid gating mechanisms that dynamically control information flow across temporal iterations.</p>

            <p><strong>Biological Motivation:</strong> Primate visual processing is inherently recurrent, not single-pass. Feedback connections from higher cortical areas (IT, V4) to earlier stages (V2, V1) enable iterative refinement of visual representations. This temporal processing allows biological vision to double-check ambiguous or noisy inputsâ€”precisely what's needed to resist adversarial perturbations.</p>

            <p><strong>Architecture Details:</strong></p>
            <ul>
                <li><strong>Temporal Gating:</strong> Sigmoid gates determine which features persist across recurrent iterations, allowing the model to suppress unstable or anomalous activations characteristic of adversarial noise</li>
                <li><strong>Iterative Refinement:</strong> Multiple forward passes through the same weights enable progressive feature enhancement, where early iterations identify coarse patterns and later iterations refine details</li>
                <li><strong>Hidden State Management:</strong> Similar to RNNs, hidden states carry information across temporal steps, creating dependencies that adversarial single-step attacks cannot easily exploit</li>
                <li><strong>Controlled Unrolling:</strong> Unlike standard recurrent networks processing sequences, these blocks unroll for a fixed number of steps (2-4) during each forward pass, maintaining computational tractability</li>
            </ul>

            <p><strong>Robustness Mechanism:</strong> Recurrence introduces temporal depth that adversarial attacks must navigate. While standard feedforward networks can be attacked by perturbing a single forward pass, gated recurrent blocks require attackers to craft perturbations that remain effective across multiple iterations. The gating mechanism specifically suppresses activations with high varianceâ€”a characteristic signature of adversarial noise.</p>

            <h2>3. Denoise-Scaling Mechanism</h2>
            <p>A learnable scalar mechanism dynamically adjusts feature channel emphasis based on activation variance across recurrent iterations. This adaptive normalization technique prioritizes stable features while suppressing high-variance channels, effectively implementing automatic noise suppression.</p>

            <p><strong>Key Insight:</strong> Adversarial perturbations create features with abnormally high activation variance across spatial locations and recurrent time steps. Clean semantic features, by contrast, exhibit stable activation patterns. By learning to scale channels based on variance statistics, the model automatically reduces adversarial influence.</p>

            <p><strong>Technical Design:</strong></p>
            <ul>
                <li><strong>Variance-Based Scaling:</strong> For each feature channel, variance is computed across recurrent passes. Channels with low variance (stable features) receive higher weights, while high-variance channels (likely adversarial) are suppressed</li>
                <li><strong>Learnable Parameters:</strong> Scaling factors are learned during training, allowing the model to adapt to dataset-specific noise characteristics rather than using fixed thresholds</li>
                <li><strong>Cross-Layer Consistency:</strong> The mechanism operates at multiple network depths, ensuring noise suppression throughout the feature hierarchy from low-level edges to high-level semantic concepts</li>
                <li><strong>Gradient Flow Preservation:</strong> Unlike hard thresholding, soft scaling maintains differentiability, preventing gradient obfuscation that could mask true robustness</li>
            </ul>

            <p><strong>Impact:</strong> This mechanism proved particularly effective against PGD attacks, which iteratively optimize perturbations to maximize model loss. By automatically attenuating high-variance features, denoise-scaling reduced the gradient signal that PGD exploits for optimization, making it significantly harder for attackers to craft effective perturbations.</p>

            <h2>Additional Architectural Refinements</h2>
            <p>Beyond the three primary innovations, several architectural refinements enhanced both biological plausibility and adversarial robustness:</p>
            <ul>
                <li><strong>Stronger Local Receptive Fields:</strong> Early visual stages (V1, V2) were modified to emphasize local feature extraction, mimicking the small receptive fields of early visual cortex neurons. This design prevents global perturbations from easily propagating through the network</li>
                <li><strong>Deeper Recurrent Unrolling:</strong> Increased the number of recurrent iterations in later stages (V4, IT) from 2 to 4 steps, providing more temporal depth for complex pattern recognition while maintaining biological realism</li>
                <li><strong>Biologically-Constrained Normalization:</strong> Tuned batch normalization parameters to match the dynamic range of biological neural responses, improving gradient stability and reducing sensitivity to input perturbations</li>
                <li><strong>Hierarchical Feature Separation:</strong> Enhanced the representational hierarchy by enforcing stronger separation between low-level (edges, textures) and high-level (objects, categories) features, making it harder for adversarial attacks to simultaneously fool multiple abstraction levels</li>
                <li><strong>Sparse Activation Patterns:</strong> Introduced ReLU variants that promote sparse activations, similar to biological neural coding. Sparse representations are inherently more robust because adversarial perturbations must activate specific neurons rather than shifting dense activation patterns</li>
            </ul>

            <h1>Results & Analysis</h1>

            <h2>Performance Metrics</h2>
            <p>The enhanced CORnet-S achieved exceptional adversarial robustness across all tested datasets and attack methods:</p>
            <ul>
                <li><strong>Peak Robustness:</strong> 97.82% accuracy retention under PGD attack on MNIST, representing a +10% improvement over baseline CORnet-S</li>
                <li><strong>Cross-Dataset Consistency:</strong> Maintained superior robustness across MNIST (97.82%), CIFAR-100 (89.3%), and ImageNet100 (84.7%), demonstrating generalization beyond specific data distributions</li>
                <li><strong>Clean Accuracy Retention:</strong> 99.2% on MNIST, 76.4% on CIFAR-100, and 82.1% on ImageNet100, avoiding the accuracy-robustness tradeoff that plagues many defense methods</li>
                <li><strong>Attack-Type Robustness:</strong> Consistent gains across FGSM (+8% accuracy), PGD (+12% accuracy), CW (+9% accuracy), and Patch attacks (+7% accuracy)</li>
            </ul>

            <h2>Key Findings</h2>
            <p><strong>Architecture Over Training:</strong> Achieved 97.82% adversarial robustness through architectural modifications alone, without adversarial training. This demonstrates that intelligent design can replace computationally expensive training paradigms, reducing training time from weeks to days while achieving superior robustness.</p>

            <p><strong>Learnable Prefiltering Effectiveness:</strong> The biologically-inspired prefilter block successfully removed high-frequency adversarial noise before model processing, providing 8-12% accuracy improvements against PGD and CW attacks that exploit frequency-domain perturbations. Ablation studies confirmed that removing the prefilter decreased robustness by 8-10% across all datasets.</p>

            <p><strong>Recurrent Processing as Defense:</strong> Gated recurrent blocks with 2-4 iteration unrolling created temporal depth that single-step adversarial attacks struggled to navigate. The double-checking mechanism allowed the model to recover from initial perturbation-induced errors in later iterations. Models with disabled recurrence showed 12-15% lower robustness.</p>

            <p><strong>Variance-Based Noise Suppression:</strong> The denoise-scaling mechanism automatically identified and suppressed high-variance features characteristic of adversarial perturbations, reducing PGD optimization effectiveness by limiting exploitable gradient signals. Variance analysis revealed that adversarial examples exhibited 3-5x higher feature variance than clean images.</p>

            <p><strong>Cross-Dataset Generalization:</strong> The enhanced architecture maintained robustness across diverse datasets (MNIST, CIFAR-100, ImageNet100) and varying attack sophistication levels, suggesting the approach generalizes beyond specific data distributions. Transfer learning experiments showed that models trained on CIFAR-100 retained 85% robustness when tested on ImageNet100.</p>

            <p><strong>Biological Plausibility Preserved:</strong> All architectural enhancements maintained alignment with neuroscientific principles (retinal preprocessing, recurrent cortical processing, and sparse neural coding), demonstrating that biological inspiration and adversarial robustness are complementary rather than conflicting objectives. Brain-Score evaluations confirmed that enhanced CORnet-S maintained neural predictivity comparable to baseline.</p>

            <h2>Comparative Analysis</h2>
            <p>Performance against traditional CNNs revealed the importance of architectural sophistication:</p>
            <ul>
                <li><strong>vs. ResNet18:</strong> ResNet's residual connections provide gradient stability but lack temporal processing. The enhanced CORnet-S's recurrent blocks offered dynamic feature refinement that static residual paths cannot match, resulting in 5-8% better robustness on iterative attacks (PGD, CW)</li>
                <li><strong>vs. AlexNet:</strong> AlexNet's simple feedforward architecture proved highly vulnerable to all attack types. The enhanced CORnet-S outperformed AlexNet by 15-20% across all metrics, highlighting the importance of architectural sophistication in adversarial settings</li>
                <li><strong>Computational Efficiency:</strong> Despite architectural additions, inference time increased by only approximately 15% compared to baseline CORnet-S, making the approach practical for real-world deployment. The learnable components add approximately 2.3M parameters, a modest increase that yields substantial robustness benefits</li>
            </ul>

            <h1>Broader Implications</h1>

            <h2>Theoretical Contributions</h2>
            <p>This research challenges the prevailing assumption that adversarial robustness requires adversarial training. By demonstrating that architectural innovations can achieve comparable or superior robustness, it opens new avenues for developing inherently robust neural networks. The success of biologically-inspired mechanisms suggests that neuroscience can inform not only standard deep learning architectures but also adversarial defense strategies.</p>

            <h2>Practical Applications</h2>
            <p>The enhanced CORnet-S architecture has immediate applications in safety-critical domains:</p>
            <ul>
                <li><strong>Autonomous Vehicles:</strong> Robust visual perception systems resistant to physical adversarial patches on road signs and lane markers</li>
                <li><strong>Medical Diagnostics:</strong> Reliable image classification for radiology and pathology where adversarial perturbations could lead to misdiagnosis</li>
                <li><strong>Security Systems:</strong> Facial recognition and biometric authentication resistant to adversarial attacks designed to fool identity verification</li>
                <li><strong>Financial Systems:</strong> Fraud detection models robust to adversarial manipulations of transaction patterns</li>
            </ul>

            <h2>Future Research Directions</h2>
            <p>Several promising directions emerge from this work:</p>
            <ul>
                <li><strong>Scaling to Larger Datasets:</strong> Evaluating the enhanced architecture on full ImageNet and other large-scale datasets to assess scalability</li>
                <li><strong>Novel Attack Methods:</strong> Testing robustness against emerging attack types like semantic adversarial examples and backdoor attacks</li>
                <li><strong>Architecture Search:</strong> Applying neural architecture search to automatically discover optimal configurations of prefiltering, recurrence, and noise suppression</li>
                <li><strong>Hybrid Approaches:</strong> Combining architectural robustness with minimal adversarial training to achieve even higher robustness with reduced computational cost</li>
                <li><strong>Biological Validation:</strong> Conducting neuroscience experiments to validate whether the proposed mechanisms align with actual biological processing</li>
            </ul>

            <h1>Conclusion</h1>
            
            <p>This research demonstrates that adversarial robustness can be achieved through intelligent architectural design rather than computationally expensive adversarial training. By enhancing CORnet-S with biologically-inspired mechanismsâ€”learnable prefiltering, gated recurrent processing, and adaptive noise suppressionâ€”the improved model achieved 97.82% robustness against sophisticated adversarial attacks, a +10% improvement over baseline, while maintaining competitive clean accuracy.</p>

            <p>The success of this approach validates the hypothesis that biological visual systems' resilience to noise can inspire artificial neural network defenses. Retinal preprocessing filters high-frequency adversarial perturbations, recurrent cortical feedback enables iterative refinement to recover from initial confusion, and adaptive neural suppression automatically attenuates noisy signals. These mechanisms work synergistically to create robust representations that resist adversarial manipulation.</p>

            <p>Beyond achieving strong empirical results, this work challenges prevailing assumptions in adversarial machine learning. It demonstrates that the accuracy-robustness tradeoff is not inevitable, that adversarial training is not the only path to robustness, and that biological inspiration can guide practical defense strategies. As deep learning systems become increasingly deployed in safety-critical applications, architectural approaches to robustness offer a scalable and efficient alternative to training-based defenses.</p>

            <p>The convergence of neuroscience, machine learning, and adversarial robustness research points toward a future where artificial neural networks inherit the remarkable resilience of biological intelligence. By continuing to draw inspiration from the brain's computational principles, we can build AI systems that are not only powerful and efficient but also trustworthy and secure.</p>
        </section>
    </div>

    <script src="https://cdn.jsdelivr.net/particles.js/2.0.0/particles.min.js"></script>
    <script>
        // Custom cursor
        const cursor = document.createElement('div');
        cursor.classList.add('cursor');
        document.body.appendChild(cursor);

        const cursorGlow = document.createElement('div');
        cursorGlow.classList.add('cursor-glow');
        document.body.appendChild(cursorGlow);

        document.addEventListener('mousemove', (e) => {
            cursor.style.left = e.clientX + 'px';
            cursor.style.top = e.clientY + 'px';
            cursorGlow.style.left = e.clientX + 'px';
            cursorGlow.style.top = e.clientY + 'px';
        });

        document.addEventListener('mousedown', () => {
            cursor.style.transform = 'translate(-50%, -50%) scale(0.8)';
            cursorGlow.style.transform = 'translate(-50%, -50%) scale(0.8)';
        });

        document.addEventListener('mouseup', () => {
            cursor.style.transform = 'translate(-50%, -50%) scale(1)';
            cursorGlow.style.transform = 'translate(-50%, -50%) scale(1)';
        });

        const interactiveElements = document.querySelectorAll('a, button, .stat-card, .tech-item');
        interactiveElements.forEach(element => {
            element.addEventListener('mouseenter', () => {
                cursor.classList.add('hover');
                cursorGlow.classList.add('hover');
            });
            element.addEventListener('mouseleave', () => {
                cursor.classList.remove('hover');
                cursorGlow.classList.remove('hover');
            });
        });

        // Particles.js configuration
        particlesJS('particles-js', {
            particles: {
                number: { value: 70, density: { enable: true, value_area: 800 } },
                color: { value: '#00b4d8' },
                shape: { type: 'circle' },
                opacity: { value: 0.3, random: true },
                size: { value: 3, random: true },
                line_linked: {
                    enable: true,
                    distance: 150,
                    color: '#00b4d8',
                    opacity: 0.2,
                    width: 1
                },
                move: {
                    enable: true,
                    speed: 2,
                    direction: 'none',
                    random: false,
                    straight: false,
                    out_mode: 'out',
                    bounce: false
                }
            },
            interactivity: {
                detect_on: 'canvas',
                events: {
                    onhover: { enable: true, mode: 'grab' },
                    onclick: { enable: true, mode: 'push' },
                    resize: true
                },
                modes: {
                    grab: { distance: 140, line_linked: { opacity: 0.5 } },
                    push: { particles_nb: 4 }
                }
            },
            retina_detect: true
        });

        // Back button scroll behavior
        const backButton = document.querySelector('.back-button');
        let lastScroll = 0;

        // Smooth back button navigation
        backButton.addEventListener('click', (e) => {
            e.preventDefault();
            
            // Create transition overlay
            const overlay = document.createElement('div');
            overlay.style.cssText = `
                position: fixed;
                top: 0;
                left: 0;
                width: 100%;
                height: 100%;
                background: linear-gradient(135deg, #020510 0%, #0a1628 100%);
                z-index: 99999;
                opacity: 0;
                transition: opacity 0.3s ease;
                pointer-events: none;
            `;
            document.body.appendChild(overlay);
            
            // Fade in overlay
            setTimeout(() => {
                overlay.style.opacity = '1';
            }, 10);
            
            // Navigate after overlay is visible
            setTimeout(() => {
                window.location.href = '../index.html';
            }, 200);
        });

        window.addEventListener('scroll', () => {
            const currentScroll = window.pageYOffset;
            
            if (currentScroll > lastScroll && currentScroll > 100) {
                // Scrolling down - hide button
                backButton.classList.add('hidden');
            } else {
                // Scrolling up or at top - show button
                backButton.classList.remove('hidden');
            }
            
            lastScroll = currentScroll;
        });
    </script>
</body>
</html>