<!DOCTYPE html>
<html lang="en" style="cursor: none !important; background-color: #020510 !important;">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Security Footage Pose Estimation System | Rishi Shah</title>
    <meta name="description" content="Real time human pose detection for surveillance applications. 17 keypoint detection using Keypoint R-CNN ResNet-50 FPN for security footage analysis.">
    <style>
        /* CRITICAL: Hide cursor and background immediately on page load */
        html, body, * {
            cursor: none !important;
        }
        html, body {
            background-color: #020510 !important;
            margin: 0;
            padding: 0;
        }
    </style>
    <link rel="prefetch" href="../index.html">
    <link rel="preload" href="../style.css" as="style">
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <script src="https://kit.fontawesome.com/17ea408dcc.js" crossorigin="anonymous"></script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        :root {
            --primary: #00b4d8;
            --secondary: #5EEAD4;
            --dark: #020510;
            --darker: #0a1628;
            --light: #e0f2fe;
            --accent: #38bdf8;
        }

        body {
            font-family: 'Poppins', sans-serif;
            background: linear-gradient(135deg, var(--dark) 0%, var(--darker) 100%);
            color: var(--light);
            line-height: 1.6;
            min-height: 100vh;
            overflow-x: hidden;
            cursor: none !important;
        }

        * {
            cursor: none !important;
        }

        #particles-js {
            position: fixed;
            width: 100%;
            height: 100%;
            top: 0;
            left: 0;
            z-index: 5;
            pointer-events: none;
        }

        .back-button {
            position: fixed;
            top: 2rem;
            left: 2rem;
            z-index: 10001;
            width: 48px;
            height: 48px;
            display: flex;
            align-items: center;
            justify-content: center;
            background: rgba(0, 180, 216, 0.1);
            border: 1px solid var(--primary);
            color: var(--primary);
            text-decoration: none;
            border-radius: 50%;
            font-size: 1.2rem;
            transition: all 0.4s cubic-bezier(0.4, 0, 0.2, 1);
            backdrop-filter: blur(10px);
            opacity: 1;
            transform: translateX(0);
        }

        .back-button.hidden {
            opacity: 0;
            transform: translateX(-20px);
            pointer-events: none;
        }

        .back-button:hover {
            background: rgba(0, 180, 216, 0.2);
            transform: scale(1.1);
            box-shadow: 0 0 30px rgba(0, 180, 216, 0.4);
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 6rem 2rem 4rem;
            position: relative;
            z-index: 10;
        }

        .project-header {
            text-align: center;
            margin-bottom: 4rem;
            padding: 3rem 2rem;
            background: rgba(10, 22, 40, 0.5);
            border-radius: 16px;
            border: 1px solid rgba(0, 180, 216, 0.2);
            position: relative;
            overflow: hidden;
        }

        .project-header::before {
            content: '';
            position: absolute;
            top: 0;
            left: -100%;
            width: 100%;
            height: 2px;
            background: linear-gradient(90deg, transparent, var(--primary), transparent);
            animation: scan 3s infinite;
        }

        @keyframes scan {
            0% { left: -100%; }
            100% { left: 100%; }
        }

        .project-header h1 {
            font-size: 3rem;
            font-weight: 700;
            margin-bottom: 1rem;
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        .project-subtitle {
            font-size: 1.1rem;
            color: var(--secondary);
            font-weight: 300;
            margin-bottom: 0.5rem;
        }

        .project-meta {
            display: flex;
            justify-content: center;
            gap: 2rem;
            margin-top: 1.5rem;
            font-size: 0.9rem;
            color: rgba(255, 255, 255, 0.6);
        }

        .section {
            margin-bottom: 2rem;
            padding: 2rem;
            background: rgba(10, 22, 40, 0.3);
            border-radius: 12px;
            border: 1px solid rgba(0, 180, 216, 0.1);
        }

        .section-header {
            font-size: 2rem;
            font-weight: 600;
            margin-bottom: 1.5rem;
            color: var(--primary);
            font-family: 'Courier New', monospace;
        }

        .highlight-box {
            background: rgba(0, 180, 216, 0.05);
            border-left: 4px solid var(--primary);
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-radius: 8px;
        }

        /* Document-style formatting */
        .document-section {
            margin-bottom: 2rem;
            padding: 3rem;
            background: rgba(10, 22, 40, 0.3);
            border-radius: 12px;
            border: 1px solid rgba(0, 180, 216, 0.1);
        }

        .document-section h1 {
            font-size: 2rem;
            font-weight: 600;
            color: var(--primary);
            font-family: 'Courier New', monospace;
            margin-bottom: 1.5rem;
            margin-top: 2.5rem;
        }

        .document-section h1:first-child {
            margin-top: 0;
        }

        .document-section h2 {
            font-size: 1.3rem;
            font-weight: 500;
            color: var(--secondary);
            margin-bottom: 1rem;
            margin-top: 2rem;
        }

        .document-section p {
            font-size: 1rem;
            line-height: 1.8;
            color: rgba(255, 255, 255, 0.85);
            margin-bottom: 1rem;
        }

        .document-section ul {
            margin: 1rem 0 1rem 1.5rem;
            list-style-type: disc;
        }

        .document-section ul li {
            font-size: 1rem;
            line-height: 1.8;
            color: rgba(255, 255, 255, 0.85);
            margin-bottom: 0.5rem;
        }

        .document-section strong {
            color: #ffffff;
            font-weight: 600;
        }

        .document-section code {
            background: rgba(0, 180, 216, 0.1);
            padding: 0.2rem 0.4rem;
            border-radius: 4px;
            font-family: 'Courier New', monospace;
            color: var(--secondary);
            font-size: 0.9em;
        }

        .document-section pre {
            background: rgba(2, 5, 16, 0.8);
            padding: 1rem;
            border-radius: 6px;
            overflow-x: auto;
            margin: 1rem 0;
        }

        .stats-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 1.5rem;
            margin: 2rem 0;
        }

        .stat-card {
            background: rgba(0, 180, 216, 0.05);
            padding: 1.5rem;
            border-radius: 12px;
            border: 1px solid rgba(0, 180, 216, 0.2);
            text-align: center;
            transition: all 0.3s ease;
        }

        .stat-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 30px rgba(0, 180, 216, 0.2);
            border-color: var(--primary);
        }

        .stat-number {
            font-size: 2.5rem;
            font-weight: 700;
            color: var(--primary);
            margin-bottom: 0.5rem;
        }

        .stat-label {
            font-size: 0.9rem;
            color: rgba(255, 255, 255, 0.7);
        }

        .tech-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 1rem;
            margin: 2rem 0;
        }

        .tech-item {
            background: rgba(0, 180, 216, 0.05);
            padding: 1rem;
            border-radius: 8px;
            border: 1px solid rgba(0, 180, 216, 0.2);
            text-align: center;
            font-weight: 500;
            transition: all 0.3s ease;
        }

        .tech-item:hover {
            background: rgba(0, 180, 216, 0.1);
            border-color: var(--primary);
            transform: scale(1.05);
        }

        @media (max-width: 768px) {
            .project-header h1 {
                font-size: 2rem;
            }

            .container {
                padding: 5rem 1rem 2rem;
            }

            .back-button {
                top: 1rem;
                left: 1rem;
                padding: 0.5rem 1rem;
            }

            .stats-grid {
                grid-template-columns: 1fr;
            }
        }

        .project-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            margin-top: 1rem;
            padding: 0.75rem 1.5rem;
            background: rgba(0, 180, 216, 0.1);
            border: 1px solid var(--primary);
            color: var(--primary);
            text-decoration: none;
            border-radius: 8px;
            transition: all 0.3s ease;
        }

        .project-link:hover {
            background: rgba(0, 180, 216, 0.2);
            box-shadow: 0 0 20px rgba(0, 180, 216, 0.3);
            transform: translateY(-2px);
        }

        /* Custom Cursor */
        .cursor {
            width: 16px;
            height: 16px;
            border: 2px solid #00b4d8;
            border-radius: 50%;
            position: fixed;
            pointer-events: none;
            z-index: 9999;
            transition: transform 0.05s ease-out, opacity 0.2s ease, width 0.2s ease, height 0.2s ease;
            transform: translate(-50%, -50%);
            box-shadow: 0 0 10px rgba(0, 180, 220, 0.5);
            opacity: 1;
            will-change: transform;
        }

        .cursor.hover {
            width: 32px;
            height: 32px;
            background: rgba(0, 180, 220, 0.1);
            border-width: 3px;
            box-shadow: 0 0 20px rgba(0, 180, 220, 0.8);
        }

        .cursor-glow {
            width: 40px;
            height: 40px;
            background: radial-gradient(circle, rgba(0, 180, 220, 0.2) 0%, transparent 70%);
            border-radius: 50%;
            position: fixed;
            pointer-events: none;
            z-index: 9998;
            transition: transform 0.1s ease-out, width 0.2s ease, height 0.2s ease;
            transform: translate(-50%, -50%);
            will-change: transform;
        }

        .cursor-glow.hover {
            width: 60px;
            height: 60px;
            background: radial-gradient(circle, rgba(0, 180, 220, 0.3) 0%, transparent 70%);
        }

        a, button, .stat-card, .tech-item {
            cursor: none !important;
        }
    </style>
</head>
<body>
    <div id="particles-js"></div>
    
    <a href="../index.html" class="back-button">
        <i class="fas fa-arrow-left"></i>
    </a>

    <div class="container">
        <!-- Box 1: Header -->
        <div class="project-header">
            <div class="project-subtitle">COMPUTER VISION & DEEP LEARNING</div>
            <h1>Security Footage Pose Estimation System</h1>
            <div class="project-subtitle">Real Time Human Pose Detection for Surveillance Applications</div>
            <div class="project-meta">
                <span>üëÅÔ∏è 17 Keypoint Detection</span>
                <span>‚ö° Real Time Processing</span>
                <span>üéØ 0.9+ Confidence</span>
            </div>
            <a href="https://github.com/RishiShah99/SecurityFootageTracker" target="_blank" class="project-link">
                <i class="fab fa-github"></i> View on GitHub
            </a>
        </div>

        <!-- Box 2: Overview -->
        <section class="section">
            <h2 class="section-header">Overview</h2>
            <div class="highlight-box">
                <p><strong>Security Footage Pose Estimation System</strong> applies state of the art pose estimation to security footage analysis, enabling automated detection and tracking of human movements in surveillance videos. Using Keypoint R-CNN, a powerful deep learning architecture, the system identifies 17 body keypoints (joints and key body parts) and overlays skeletal structures onto video frames in real time.</p>
            </div>
            <p>The system processes security footage to extract meaningful motion data, which can be used for activity recognition, anomaly detection, crowd analysis, and behavioral pattern identification. By visualizing human poses with connected keypoints, the system makes complex motion data immediately interpretable for security personnel.</p>
            
            <p style="margin-top: 1rem;">Built on PyTorch's pre trained Keypoint R-CNN ResNet-50 FPN model, the system achieves high confidence detection (0.9+) and generates color coded skeletal overlays that make complex motion patterns immediately visible to security operators.</p>
        </section>

        <!-- Box 3: Technology Stack -->
        <section class="section">
            <h2 class="section-header">Technology Stack</h2>
            <div class="tech-grid">
                <div class="tech-item">Python</div>
                <div class="tech-item">PyTorch</div>
                <div class="tech-item">torchvision</div>
                <div class="tech-item">Keypoint R-CNN</div>
                <div class="tech-item">OpenCV</div>
                <div class="tech-item">Flask</div>
                <div class="tech-item">NumPy</div>
                <div class="tech-item">Pillow</div>
                <div class="tech-item">CUDA</div>
            </div>
        </section>

        <!-- Box 4: Main Document Content -->
        <section class="document-section">
            <h1>Architecture & Model Design</h1>

            <h2>Keypoint R-CNN Foundation</h2>
            <p>Keypoint R-CNN extends Mask R-CNN by adding a keypoint detection head alongside object detection and instance segmentation capabilities. The architecture uses ResNet-50 with Feature Pyramid Network (FPN) as its backbone for multi scale feature extraction, enabling detection at various resolutions and distances.</p>

            <p>The Feature Pyramid Network creates a multi level feature hierarchy that captures both fine grained details (for close subjects) and high level semantic information (for distant subjects). This pyramid structure enables robust detection across varying subject distances, from close range surveillance to wide angle crowd monitoring.</p>

            <p>Unlike simple classification networks that output single labels, Keypoint R-CNN performs instance segmentation, identifying individual people in crowded scenes and assigning each person their own set of 17 keypoint coordinates. This multi person capability is critical for surveillance applications where multiple subjects often appear simultaneously.</p>

            <h2>COCO Keypoint Standard</h2>
            <p>The model detects 17 keypoints following the COCO (Common Objects in Context) dataset standard. These keypoints represent major anatomical joints and facial features:</p>

            <ul>
                <li><strong>Head Region (5 points):</strong> Nose, left eye, right eye, left ear, right ear</li>
                <li><strong>Upper Body (4 points):</strong> Left shoulder, right shoulder, left elbow, right elbow</li>
                <li><strong>Arms (2 points):</strong> Left wrist, right wrist</li>
                <li><strong>Lower Body (4 points):</strong> Left hip, right hip, left knee, right knee</li>
                <li><strong>Legs (2 points):</strong> Left ankle, right ankle</li>
            </ul>

            <p>Each keypoint detection includes pixel coordinates (x, y) and a confidence score ranging from 0 to 1. High confidence scores (>0.9) indicate reliable detections, while lower scores suggest occlusion, poor lighting, or ambiguous poses. The system filters low confidence detections to ensure visual clarity and prevent false positives.</p>

            <h2>Skeletal Connection Mapping</h2>
            <p>Connecting the 17 keypoints creates a skeletal overlay representing human body structure. The system draws 15 edges following anatomical connections:</p>

            <ul>
                <li>Face structure: nose to eyes, eyes to ears</li>
                <li>Upper body: shoulders to elbows, elbows to wrists</li>
                <li>Torso: shoulders to hips</li>
                <li>Lower body: hips to knees, knees to ankles</li>
            </ul>

            <p>Each edge is color coded using HSV (Hue, Saturation, Value) to RGB conversion, generating visually distinct colors for different body parts. This color mapping helps operators quickly identify specific limbs and track movement patterns, especially in crowded scenes where multiple skeletal overlays intersect.</p>

            <h1>Technical Implementation</h1>

            <h2>Video Processing Pipeline</h2>
            <p>The system processes video footage frame by frame using OpenCV for video I/O and PyTorch for inference. Each frame undergoes the following pipeline:</p>

            <ul>
                <li><strong>Frame Extraction:</strong> OpenCV reads video frames sequentially, maintaining original resolution and color space</li>
                <li><strong>Preprocessing:</strong> Frames convert to PyTorch tensors with normalized pixel values (0 to 1 range) and channel ordering (RGB)</li>
                <li><strong>Inference:</strong> Keypoint R-CNN processes tensors through GPU accelerated forward pass, outputting bounding boxes, keypoint coordinates, and confidence scores</li>
                <li><strong>Filtering:</strong> Detections below 0.9 confidence threshold are discarded to ensure accuracy</li>
                <li><strong>Visualization:</strong> Keypoints rendered as green circles (3px radius), skeletal edges drawn as colored lines (2px thickness)</li>
                <li><strong>Output:</strong> Annotated frames written to output video file with identical codec and frame rate as input</li>
            </ul>

            <p>The pipeline processes videos at approximately 15 to 30 FPS on modern GPUs (NVIDIA RTX series), enabling near real time analysis for most surveillance footage. Progress tracking via tqdm library provides visual feedback during batch processing.</p>

            <h2>Keypoint Visualization Algorithm</h2>
            <p>The <code>draw_keypoints</code> function handles overlay generation. For each detected person:</p>

            <ul>
                <li>Extract keypoint tensor containing 17 coordinate pairs and visibility flags</li>
                <li>Filter keypoints based on confidence threshold (>0.9)</li>
                <li>Draw green circles at each valid keypoint location using OpenCV's <code>circle</code> function</li>
                <li>Iterate through 15 skeletal connections, checking if both endpoint keypoints are visible</li>
                <li>Generate unique color for each edge using HSV to RGB conversion (hue varies per edge index)</li>
                <li>Draw colored line between connected keypoints using OpenCV's <code>line</code> function</li>
            </ul>

            <p>The HSV color generation ensures maximum visual distinction. By varying hue while keeping saturation and value constant, each skeletal edge receives a perceptually different color. This is superior to random RGB values which can produce similar looking colors for adjacent body parts.</p>

            <h2>Flask Web Interface</h2>
            <p>Beyond batch video processing, the system includes a Flask web application for single image pose estimation. This interface provides quick testing and demonstration capabilities without processing entire video files.</p>

            <p><strong>Upload Endpoint (<code>POST /upload</code>):</strong> Accepts multipart form data containing an image file. The endpoint validates file type (JPEG, PNG), saves to temporary storage, loads image using Pillow, converts to tensor, runs inference, and renders keypoint overlay.</p>

            <p><strong>Result Storage:</strong> Annotated images save to a <code>/results</code> directory with timestamped filenames. This creates an archive of processed images for later review or comparison. The endpoint returns the annotated image as a downloadable file in the HTTP response.</p>

            <p><strong>CORS Configuration:</strong> Cross origin resource sharing headers enable frontend JavaScript applications to upload images from different domains. This facilitates integration into web based dashboards or mobile applications.</p>

            <h1>Performance Optimization</h1>

            <h2>GPU Acceleration</h2>
            <p>PyTorch's CUDA integration enables GPU acceleration for both preprocessing and inference. The model loads onto GPU memory at application startup, avoiding repeated CPU to GPU transfers. Batch processing (though typically batch size 1 for video frames) maximizes GPU utilization.</p>

            <p>For systems without CUDA compatible GPUs, the code gracefully falls back to CPU inference. While significantly slower (5 to 10x reduction in throughput), CPU mode still enables offline processing of pre recorded footage where real time performance is not required.</p>

            <h2>Confidence Threshold Tuning</h2>
            <p>The 0.9 confidence threshold balances precision and recall. Lower thresholds (0.7 to 0.8) increase detection rate but introduce false positives from partial occlusions or background clutter. Higher thresholds (0.95+) maximize precision but miss legitimate detections in challenging lighting or pose angles.</p>

            <p>Empirical testing on diverse surveillance footage found 0.9 provides optimal balance for security applications where false positives distract operators but missed detections reduce system utility. The threshold is configurable via command line argument, allowing adaptation to specific use cases.</p>

            <h1>Real World Applications</h1>

            <h2>Security & Surveillance</h2>
            <p>The primary application is automated surveillance monitoring. Security operators can process hours of footage rapidly, with the system highlighting periods of human activity. Unusual postures (falling, crouching, altercations) become visually obvious through skeletal overlays.</p>

            <p>Anomaly detection algorithms can analyze skeletal coordinates to identify suspicious behavior. For example, repeated pacing patterns, sudden pose changes, or proximity clustering between individuals. These patterns trigger alerts for operator review rather than requiring continuous manual monitoring.</p>

            <h2>Crowd Analysis & Flow Optimization</h2>
            <p>In public spaces (airports, stadiums, retail), the system tracks crowd movement patterns. By aggregating skeletal pose data across multiple people, analysts identify bottlenecks, common pathways, and congregation zones. This informs facility layout optimization and emergency evacuation planning.</p>

            <p>Queue management systems use pose estimation to count people in lines and estimate wait times. Head orientation (detected via facial keypoints) indicates attention direction, helping understand customer engagement with displays or signage.</p>

            <h2>Sports & Athletic Performance</h2>
            <p>Coaches analyze athlete technique by reviewing skeletal overlays frame by frame. Joint angles during specific movements (basketball shots, golf swings, running stride) are measurable from keypoint coordinates. Comparing elite athletes to trainees reveals form differences and improvement opportunities.</p>

            <p>Injury prevention systems monitor repetitive stress on joints. Asymmetric poses or excessive joint extension detected through keypoint analysis trigger ergonomic warnings, particularly valuable in manufacturing or warehouse environments with repetitive manual labor.</p>

            <h2>Healthcare & Patient Monitoring</h2>
            <p>Fall detection systems use pose estimation to identify sudden transitions from standing to prone positions. Hospital corridors and elderly care facilities benefit from automated alerts when residents fall, reducing response time for medical assistance.</p>

            <p>Physical therapy tracking monitors patient mobility improvement over time. Comparing range of motion (measured through keypoint angles) across therapy sessions quantifies recovery progress objectively rather than relying solely on subjective assessments.</p>

            <h2>Retail Analytics</h2>
            <p>Store layout optimization analyzes customer movement patterns. Heatmaps generated from skeletal tracking show which aisles receive most traffic and where customers pause (indicating interest). This data informs product placement and promotional display positioning.</p>

            <p>Engagement metrics measure how long customers stand in front of specific products. Combined with facial keypoint orientation, systems detect whether customers are actively examining items or merely passing through, providing finer grained analytics than simple foot traffic counts.</p>

            <h1>Challenges & Limitations</h1>

            <h2>Occlusion Handling</h2>
            <p>Partial occlusions (people behind furniture, obscured by other individuals) reduce keypoint detection accuracy. The model attempts to infer occluded keypoints based on visible portions, but confidence scores drop significantly. Heavy occlusion causes complete detection failures.</p>

            <p>Multi camera fusion mitigates occlusion by combining views from different angles. If one camera's view is blocked, another may have clear line of sight. Geometric constraints ensure keypoints from different views correspond to the same person through triangulation.</p>

            <h2>Low Light & Image Quality</h2>
            <p>Security footage often suffers from poor lighting, compression artifacts, and low resolution. These factors degrade model performance, as the pre trained Keypoint R-CNN was optimized for high quality images. Fine tuning on domain specific footage (actual surveillance cameras from target deployment) improves robustness.</p>

            <p>Image enhancement preprocessing (contrast adjustment, noise reduction, super resolution upscaling) can improve detection rates. However, these techniques add computational overhead and may introduce artifacts that confuse the model.</p>

            <h2>Computational Requirements</h2>
            <p>Real time processing requires substantial GPU resources. A single RTX 3090 handles approximately 30 FPS at 1080p resolution. Multi camera deployments (10+ streams) necessitate distributed processing across multiple servers or edge compute devices at each camera location.</p>

            <p>Edge deployment trades accuracy for efficiency by using smaller models (MobileNet backbones, quantized weights) that run on lower power devices. This enables in camera pose estimation with minimal network bandwidth, though detection performance degrades compared to full ResNet-50 models.</p>

            <h1>Future Enhancements</h1>

            <h2>Temporal Tracking & Action Recognition</h2>
            <p>Current implementation processes frames independently without temporal continuity. Adding person tracking across frames enables action recognition (walking, running, fighting, falling). Recurrent neural networks or temporal convolutional networks analyze keypoint sequences to classify activities.</p>

            <p>Optical flow integration improves tracking robustness by predicting where people will appear in subsequent frames based on motion vectors. This reduces false identifications when multiple people cross paths or move close together.</p>

            <h2>Custom Dataset Fine Tuning</h2>
            <p>The pre trained COCO model generalizes well but can improve through domain specific fine tuning. Collecting annotated surveillance footage from actual deployment environments and retraining the keypoint detection head adapts the model to specific camera angles, lighting conditions, and subject demographics.</p>

            <p>Active learning pipelines identify challenging frames where the model has low confidence and prioritize them for manual annotation. This focuses labeling effort on edge cases that improve model robustness most effectively.</p>

            <h2>Privacy Preserving Approaches</h2>
            <p>Skeletal pose data is less privacy invasive than raw video, as it discards facial features and clothing details. Processing video locally to extract only keypoint coordinates before storage or transmission protects subject privacy while retaining behavioral analytics capabilities.</p>

            <p>Differential privacy techniques add controlled noise to keypoint coordinates, preventing individual identification while preserving statistical patterns useful for crowd analysis. This enables public space monitoring without creating invasive surveillance systems.</p>

            <h1>Conclusion</h1>

            <p>The Security Footage Pose Estimation System demonstrates how pre trained computer vision models can be effectively deployed for domain specific applications. By leveraging PyTorch's Keypoint R-CNN trained on the massive COCO dataset, the system achieves robust pose detection without requiring extensive custom dataset collection or model training.</p>

            <p>From technical implementation perspective, the project showcases complete ML deployment pipeline: model loading and GPU optimization, video processing with OpenCV, visualization overlay generation, and web interface development with Flask. Each component is production ready, with error handling, performance optimization, and configurable parameters.</p>

            <p>The real world applications span security, healthcare, sports, retail, and urban planning, proving pose estimation's versatility beyond research settings. As edge compute hardware improves and models become more efficient, these capabilities will increasingly embed into camera infrastructure itself, enabling ubiquitous intelligent video analysis while respecting privacy through on device processing and skeletal abstraction.</p>
        </section>
    </div>

    <script src="https://cdn.jsdelivr.net/particles.js/2.0.0/particles.min.js"></script>
    <script>
        // Custom cursor
        const cursor = document.createElement('div');
        cursor.classList.add('cursor');
        document.body.appendChild(cursor);

        const cursorGlow = document.createElement('div');
        cursorGlow.classList.add('cursor-glow');
        document.body.appendChild(cursorGlow);

        document.addEventListener('mousemove', (e) => {
            cursor.style.left = e.clientX + 'px';
            cursor.style.top = e.clientY + 'px';
            cursorGlow.style.left = e.clientX + 'px';
            cursorGlow.style.top = e.clientY + 'px';
        });

        document.addEventListener('mousedown', () => {
            cursor.style.transform = 'translate(-50%, -50%) scale(0.8)';
            cursorGlow.style.transform = 'translate(-50%, -50%) scale(0.8)';
        });

        document.addEventListener('mouseup', () => {
            cursor.style.transform = 'translate(-50%, -50%) scale(1)';
            cursorGlow.style.transform = 'translate(-50%, -50%) scale(1)';
        });

        const interactiveElements = document.querySelectorAll('a, button, .stat-card, .tech-item');
        interactiveElements.forEach(element => {
            element.addEventListener('mouseenter', () => {
                cursor.classList.add('hover');
                cursorGlow.classList.add('hover');
            });
            element.addEventListener('mouseleave', () => {
                cursor.classList.remove('hover');
                cursorGlow.classList.remove('hover');
            });
        });

        // Particles.js configuration
        particlesJS('particles-js', {
            particles: {
                number: { value: 70, density: { enable: true, value_area: 800 } },
                color: { value: '#00b4d8' },
                shape: { type: 'circle' },
                opacity: { value: 0.3, random: true },
                size: { value: 3, random: true },
                line_linked: {
                    enable: true,
                    distance: 150,
                    color: '#00b4d8',
                    opacity: 0.2,
                    width: 1
                },
                move: {
                    enable: true,
                    speed: 2,
                    direction: 'none',
                    random: false,
                    straight: false,
                    out_mode: 'out',
                    bounce: false
                }
            },
            interactivity: {
                detect_on: 'canvas',
                events: {
                    onhover: { enable: true, mode: 'grab' },
                    onclick: { enable: true, mode: 'push' },
                    resize: true
                },
                modes: {
                    grab: { distance: 140, line_linked: { opacity: 0.5 } },
                    push: { particles_nb: 4 }
                }
            },
            retina_detect: true
        });

        // Back button scroll behavior
        const backButton = document.querySelector('.back-button');
        let lastScroll = 0;

        // Smooth back button navigation
        backButton.addEventListener('click', (e) => {
            e.preventDefault();
            
            // Create transition overlay
            const overlay = document.createElement('div');
            overlay.style.cssText = `
                position: fixed;
                top: 0;
                left: 0;
                width: 100%;
                height: 100%;
                background: linear-gradient(135deg, #020510 0%, #0a1628 100%);
                z-index: 99999;
                opacity: 0;
                transition: opacity 0.3s ease;
                pointer-events: none;
            `;
            document.body.appendChild(overlay);
            
            // Fade in overlay
            setTimeout(() => {
                overlay.style.opacity = '1';
            }, 10);
            
            // Navigate after overlay is visible
            setTimeout(() => {
                window.location.href = '../index.html';
            }, 200);
        });

        window.addEventListener('scroll', () => {
            const currentScroll = window.pageYOffset;
            
            if (currentScroll > lastScroll && currentScroll > 100) {
                // Scrolling down - hide button
                backButton.classList.add('hidden');
            } else {
                // Scrolling up or at top - show button
                backButton.classList.remove('hidden');
            }
            
            lastScroll = currentScroll;
        });
    </script>
</body>
</html>