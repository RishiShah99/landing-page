<!DOCTYPE html>
<html lang="en" style="cursor: none !important; background-color: #020510 !important;">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Adaptive Learning Robot - Natural Language Robot Control | Rishi Shah</title>
    <meta name="description" content="Hack the North Top 32 - Real-time natural language robot control using Cohere LLMs, Tavily Search, and ESP32. System 1 & System 2 thinking for instant robotic skill learning.">
    <style>
        /* CRITICAL: Hide cursor and background immediately on page load */
        html, body, * {
            cursor: none !important;
        }
        html, body {
            background-color: #020510 !important;
            margin: 0;
            padding: 0;
        }
    </style>
    <link rel="prefetch" href="../index.html">
    <link rel="preload" href="../style.css" as="style">
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <script src="https://kit.fontawesome.com/17ea408dcc.js" crossorigin="anonymous"></script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        :root {
            --primary: #00b4d8;
            --secondary: #5EEAD4;
            --dark: #020510;
            --darker: #0a1628;
            --light: #e0f2fe;
            --accent: #38bdf8;
        }

        body {
            font-family: 'Poppins', sans-serif;
            background: linear-gradient(135deg, var(--dark) 0%, var(--darker) 100%);
            color: var(--light);
            line-height: 1.6;
            min-height: 100vh;
            overflow-x: hidden;
            cursor: none !important;
        }

        * {
            cursor: none !important;
        }

        #particles-js {
            position: fixed;
            width: 100%;
            height: 100%;
            top: 0;
            left: 0;
            z-index: 5;
            pointer-events: none;
        }

        .back-button {
            position: fixed;
            top: 2rem;
            left: 2rem;
            z-index: 10001;
            width: 48px;
            height: 48px;
            display: flex;
            align-items: center;
            justify-content: center;
            background: rgba(0, 180, 216, 0.1);
            border: 1px solid var(--primary);
            color: var(--primary);
            text-decoration: none;
            border-radius: 50%;
            font-size: 1.2rem;
            transition: all 0.4s cubic-bezier(0.4, 0, 0.2, 1);
            backdrop-filter: blur(10px);
            opacity: 1;
            transform: translateX(0);
        }

        .back-button.hidden {
            opacity: 0;
            transform: translateX(-20px);
            pointer-events: none;
        }

        .back-button:hover {
            background: rgba(0, 180, 216, 0.2);
            transform: scale(1.1);
            box-shadow: 0 0 30px rgba(0, 180, 216, 0.4);
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 6rem 2rem 4rem;
            position: relative;
            z-index: 10;
        }

        .project-header {
            text-align: center;
            margin-bottom: 4rem;
            padding: 3rem 2rem;
            background: rgba(10, 22, 40, 0.5);
            border-radius: 16px;
            border: 1px solid rgba(0, 180, 216, 0.2);
            position: relative;
            overflow: hidden;
        }

        .project-header::before {
            content: '';
            position: absolute;
            top: 0;
            left: -100%;
            width: 100%;
            height: 2px;
            background: linear-gradient(90deg, transparent, var(--primary), transparent);
            animation: scan 3s infinite;
        }

        @keyframes scan {
            0% { left: -100%; }
            100% { left: 100%; }
        }

        .project-header h1 {
            font-size: 3rem;
            font-weight: 700;
            margin-bottom: 1rem;
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        .project-subtitle {
            font-size: 1.1rem;
            color: var(--secondary);
            font-weight: 300;
            margin-bottom: 0.5rem;
        }

        .project-meta {
            display: flex;
            justify-content: center;
            gap: 2rem;
            margin-top: 1.5rem;
            font-size: 0.9rem;
            color: rgba(255, 255, 255, 0.6);
        }

        .section {
            margin-bottom: 2rem;
            padding: 2rem;
            background: rgba(10, 22, 40, 0.3);
            border-radius: 12px;
            border: 1px solid rgba(0, 180, 216, 0.1);
        }

        .section-header {
            font-size: 2rem;
            font-weight: 600;
            margin-bottom: 1.5rem;
            color: var(--primary);
            font-family: 'Courier New', monospace;
        }

        .highlight-box {
            background: rgba(0, 180, 216, 0.05);
            border-left: 4px solid var(--primary);
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-radius: 8px;
        }

        /* Document-style formatting */
        .document-section {
            margin-bottom: 2rem;
            padding: 3rem;
            background: rgba(10, 22, 40, 0.3);
            border-radius: 12px;
            border: 1px solid rgba(0, 180, 216, 0.1);
        }

        .document-section h1 {
            font-size: 2rem;
            font-weight: 600;
            color: var(--primary);
            font-family: 'Courier New', monospace;
            margin-bottom: 1.5rem;
            margin-top: 2.5rem;
        }

        .document-section h1:first-child {
            margin-top: 0;
        }

        .document-section h2 {
            font-size: 1.3rem;
            font-weight: 500;
            color: var(--secondary);
            margin-bottom: 1rem;
            margin-top: 2rem;
        }

        .document-section p {
            font-size: 1rem;
            line-height: 1.8;
            color: rgba(255, 255, 255, 0.85);
            margin-bottom: 1rem;
        }

        .document-section ul {
            margin: 1rem 0 1rem 1.5rem;
            list-style-type: disc;
        }

        .document-section ul li {
            font-size: 1rem;
            line-height: 1.8;
            color: rgba(255, 255, 255, 0.85);
            margin-bottom: 0.5rem;
        }

        .document-section strong {
            color: #ffffff;
            font-weight: 600;
        }

        .document-section code {
            background: rgba(0, 180, 216, 0.1);
            padding: 0.2rem 0.4rem;
            border-radius: 4px;
            font-family: 'Courier New', monospace;
            color: var(--secondary);
            font-size: 0.9em;
        }

        .video-container {
            position: relative;
            padding-bottom: 56.25%;
            height: 0;
            overflow: hidden;
            margin: 2rem 0;
            border-radius: 12px;
            border: 2px solid rgba(0, 180, 216, 0.3);
        }

        .video-container iframe {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
        }

        .stats-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 1.5rem;
            margin: 2rem 0;
        }

        .stat-card {
            background: rgba(0, 180, 216, 0.05);
            padding: 1.5rem;
            border-radius: 12px;
            border: 1px solid rgba(0, 180, 216, 0.2);
            text-align: center;
            transition: all 0.3s ease;
        }

        .stat-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 30px rgba(0, 180, 216, 0.2);
            border-color: var(--primary);
        }

        .stat-number {
            font-size: 2.5rem;
            font-weight: 700;
            color: var(--primary);
            margin-bottom: 0.5rem;
        }

        .stat-label {
            font-size: 0.9rem;
            color: rgba(255, 255, 255, 0.7);
        }

        .tech-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 1rem;
            margin: 2rem 0;
        }

        .tech-item {
            background: rgba(0, 180, 216, 0.05);
            padding: 1rem;
            border-radius: 8px;
            border: 1px solid rgba(0, 180, 216, 0.2);
            text-align: center;
            font-weight: 500;
            transition: all 0.3s ease;
        }

        .tech-item:hover {
            background: rgba(0, 180, 216, 0.1);
            border-color: var(--primary);
            transform: scale(1.05);
        }

        @media (max-width: 768px) {
            .project-header h1 {
                font-size: 2rem;
            }

            .container {
                padding: 5rem 1rem 2rem;
            }

            .back-button {
                top: 1rem;
                left: 1rem;
                padding: 0.5rem 1rem;
            }

            .stats-grid {
                grid-template-columns: 1fr;
            }
        }

        .project-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            margin-top: 1rem;
            padding: 0.75rem 1.5rem;
            background: rgba(0, 180, 216, 0.1);
            border: 1px solid var(--primary);
            color: var(--primary);
            text-decoration: none;
            border-radius: 8px;
            transition: all 0.3s ease;
        }

        .project-link:hover {
            background: rgba(0, 180, 216, 0.2);
            box-shadow: 0 0 20px rgba(0, 180, 216, 0.3);
            transform: translateY(-2px);
        }

        /* Custom Cursor */
        .cursor {
            width: 16px;
            height: 16px;
            border: 2px solid #00b4d8;
            border-radius: 50%;
            position: fixed;
            pointer-events: none;
            z-index: 9999;
            transition: transform 0.05s ease-out, opacity 0.2s ease, width 0.2s ease, height 0.2s ease;
            transform: translate(-50%, -50%);
            box-shadow: 0 0 10px rgba(0, 180, 220, 0.5);
            opacity: 1;
            will-change: transform;
        }

        .cursor.hover {
            width: 32px;
            height: 32px;
            background: rgba(0, 180, 220, 0.1);
            border-width: 3px;
            box-shadow: 0 0 20px rgba(0, 180, 220, 0.8);
        }

        .cursor-glow {
            width: 40px;
            height: 40px;
            background: radial-gradient(circle, rgba(0, 180, 220, 0.2) 0%, transparent 70%);
            border-radius: 50%;
            position: fixed;
            pointer-events: none;
            z-index: 9998;
            transition: transform 0.1s ease-out, width 0.2s ease, height 0.2s ease;
            transform: translate(-50%, -50%);
            will-change: transform;
        }

        .cursor-glow.hover {
            width: 60px;
            height: 60px;
            background: radial-gradient(circle, rgba(0, 180, 220, 0.3) 0%, transparent 70%);
        }

        a, button, .stat-card, .tech-item {
            cursor: none !important;
        }
    </style>
</head>
<body>
    <div id="particles-js"></div>
    
    <a href="../index.html" class="back-button">
        <i class="fas fa-arrow-left"></i>
    </a>

    <div class="container">
        <!-- Box 1: Header -->
        <div class="project-header">
            <div class="project-subtitle">HACK THE NORTH TOP 32 FINALIST</div>
            <h1>Adaptive Learning Robot</h1>
            <div class="project-subtitle">Natural Language Robot Control with System 1 & 2 Thinking</div>
            <div class="project-meta">
                <span>üèÜ Top 32 at Hack the North</span>
                <span>‚ö° Built in 36 Hours</span>
                <span>ü§ñ Real-time Skill Learning</span>
            </div>
            <a href="https://devpost.com/software/monkey-see-monkey-do-joc1fm" target="_blank" class="project-link">
                <i class="fas fa-external-link-alt"></i> View on Devpost
            </a>
        </div>

        <!-- Box 2: Overview -->
        <section class="section">
            <h2 class="section-header">Overview</h2>
            
            <div class="video-container">
                <iframe 
                    id="project-video"
                    src="https://www.youtube.com/embed/o5f06JSGZW8" 
                    title="Adaptive Learning Robot Demo" 
                    frameborder="0" 
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
                    allowfullscreen>
                </iframe>
            </div>
            
            <div class="highlight-box">
                <p><strong>Adaptive Learning Robot</strong> revolutionizes robotics by eliminating the need for massive datasets and months of training. Instead of reinforcement learning, users can teach a robot instantly through natural language commands. Inspired by Daniel Kahneman's dual-system thinking framework, the system combines reasoning ("System 2") with execution ("System 1") to translate unstructured web data into precise robotic movements in real-time.</p>
            </div>
            <p>Traditional robotics requires extensive training datasets and complex reinforcement learning pipelines. This project flips that paradigm on its head: type "learn the jab punch" and watch as the system searches the internet, reasons about the motion, and executes it on custom hardware‚Äîall in real-time. A chat interface combined with a 3D robot viewer makes the entire process transparent, simple, and interactive.</p>
        </section>

        <!-- Box 3: Technology Stack -->
        <section class="section">
            <h2 class="section-header">Technology Stack</h2>
            <div class="tech-grid">
                <div class="tech-item">Next.js</div>
                <div class="tech-item">React</div>
                <div class="tech-item">Three.js</div>
                <div class="tech-item">Flask</div>
                <div class="tech-item">Python</div>
                <div class="tech-item">Cohere API</div>
                <div class="tech-item">Tavily Search</div>
                <div class="tech-item">Socket.IO</div>
                <div class="tech-item">ESP32</div>
                <div class="tech-item">Arduino UNO</div>
                <div class="tech-item">TFT Display</div>
                <div class="tech-item">Servo Motors</div>
            </div>
        </section>

        <!-- Box 4: Main Document Content -->
        <section class="document-section">
            <h1>The Problem with Traditional Robotics</h1>

            <h2>Training Paradigm Limitations</h2>
            <p>Modern robotics relies heavily on reinforcement learning and supervised learning approaches that require massive datasets and extensive training time. Teaching a robot to perform a simple task like picking up an object can require thousands of training examples, weeks of computational time, and specialized expertise in machine learning pipelines.</p>

            <p>This approach creates several critical barriers:</p>
            <ul>
                <li><strong>Data Collection Overhead:</strong> Gathering sufficient training data is labor intensive, often requiring manual teleoperation or simulation environments that don't transfer well to real world conditions</li>
                <li><strong>Computational Cost:</strong> Training deep reinforcement learning models demands expensive GPU clusters and can take days to weeks for convergence</li>
                <li><strong>Inflexibility:</strong> Once trained, robots struggle to generalize to new tasks without complete retraining, making them rigid and task specific</li>
                <li><strong>Expertise Barrier:</strong> Configuring and training robotic systems requires deep expertise in machine learning, robotics, and control theory, limiting accessibility to specialized researchers</li>
            </ul>

            <p>These limitations mean that robotics remains largely confined to controlled industrial environments with repetitive, well defined tasks. Adaptive, on the spot learning, the hallmark of human intelligence, remains elusive.</p>

            <h2>The Vision: Zero-Shot Robot Learning</h2>
            <p>What if robots could learn skills instantly by reading about them, just as humans do? A martial arts student doesn't need 10,000 training examples to learn a jab punch‚Äîthey read a description, watch a demonstration, and begin practicing immediately. Adaptive Learning Robot brings this capability to robotics.</p>

            <p>The core insight: the internet already contains detailed descriptions of virtually every physical movement humans perform. From martial arts to dance, from industrial assembly to surgical procedures, web resources provide step by step motion guides. The challenge is transforming this unstructured text into precise robotic commands.</p>

            <h1>Dual-System Architecture</h1>

            <h2>System 2: Reasoning Layer</h2>
            <p>Inspired by Daniel Kahneman's "Thinking, Fast and Slow," the reasoning layer implements deliberate, analytical processing that transforms natural language into structured motion plans. This is analogous to System 2 thinking in humans, slow, methodical, and logical.</p>

            <p><strong>Pipeline Overview:</strong></p>
            <ul>
                <li><strong>Natural Language Input:</strong> User provides a command like "learn the jab punch" or "perform a wave motion" through a Next.js web interface</li>
                <li><strong>Tavily Search API:</strong> Queries the web for relevant movement descriptions, tutorials, and biomechanical data. Returns multiple sources with varying quality and formats</li>
                <li><strong>Cohere Agent Processing:</strong> Large language model analyzes search results, extracts relevant motion parameters, validates consistency across sources, and synthesizes a coherent motion guide</li>
                <li><strong>Motion Guide Generation:</strong> Structured JSON representation defining joint angles, timing sequences, velocity profiles, and coordinate frames for the requested motion</li>
            </ul>

            <p><strong>LLM Reasoning Process:</strong> The Cohere agent doesn't simply extract data, it performs multi step reasoning. It identifies anatomical landmarks mentioned in text ("extend the arm forward"), maps these to robotic joint configurations, resolves ambiguities by cross referencing multiple sources, and generates safety constraints to prevent mechanical damage. This reasoning process mirrors how humans understand and internalize movement descriptions.</p>

            <p><strong>Handling Noisy Data:</strong> Web search results are inherently messy, contradictory information, irrelevant context, imprecise language. The LLM filters noise by validating kinematic feasibility (can the robot physically achieve these angles?), checking temporal consistency (do movement phases follow logical sequences?), and prioritizing authoritative sources (instructional content over casual mentions).</p>

            <h2>System 1: Execution Layer</h2>
            <p>Once the reasoning layer produces a motion guide, the execution layer handles fast, automatic, low level control, analogous to System 1 thinking in humans. This is where abstract plans become physical reality.</p>

            <p><strong>SkillCompiler:</strong> Converts high level motion guides into timestamped servo commands. This involves inverse kinematics to map desired end effector positions to joint angles, trajectory smoothing to ensure fluid motion without jerky transitions, velocity profiling to maintain consistent speed, and safety constraint enforcement (angle limits, collision avoidance).</p>

            <p><strong>RobotControlGenerator:</strong> Translates servo commands into ESP32 compatible binary protocols. Manages dual 3 DOF robotic arms with synchronized timing, implements differential control for smooth bilateral movements, and handles communication protocol with hardware (Socket.IO over WiFi for flexibility).</p>

            <p><strong>Hardware Execution:</strong> ESP32 microcontroller receives servo sequences and controls 6 servos (3 per arm) with precise timing. Real time feedback loop confirms execution, detects mechanical resistance, and adjusts timing to maintain synchronization. Arduino UNO with TFT display visualizes robot state through an animated monkey avatar, providing user feedback on operational status.</p>

            <p><strong>Synchronization Mechanisms:</strong> Maintaining synchronization between frontend visualization, Flask backend reasoning, and ESP32 hardware execution required careful engineering. Timestamp based command queuing ensures consistent timing despite network latency. Socket.IO acknowledgment messages confirm receipt and execution of commands. Predictive visualization shows expected motion based on command queue, while actual hardware sends position updates to correct drift.</p>

            <h1>Technical Implementation</h1>

            <h2>Frontend: Next.js & Three.js Visualization</h2>
            <p>The web interface provides real time visualization and control through a modern React based stack:</p>

            <p><strong>Chat Interface:</strong> Users enter natural language commands in a conversational UI. The system displays reasoning progress, showing search results, extracted motion parameters, and generated servo sequences. This transparency helps users understand what the robot is "thinking" and builds trust in the autonomous system.</p>

            <p><strong>3D Robot Viewer:</strong> Three.js renders a real time 3D model of the dual arm robot. As motion guides are generated, the viewer shows predicted movements before hardware execution. Users can rotate the view, zoom into specific joints, and visualize trajectory paths. This enables "safety rehearsal", validating motions visually before committing to physical execution.</p>

            <p><strong>Real Time Updates:</strong> Socket.IO client maintains bidirectional communication with Flask backend. Displays live servo positions, command queue status, and reasoning pipeline progress. Latency measurements show roundtrip time from command to execution, typically 200 to 500ms depending on LLM inference time.</p>

            <h2>Backend: Flask & Cohere Integration</h2>
            <p>The Flask server orchestrates the reasoning pipeline and manages communication between frontend and hardware:</p>

            <p><strong>Tavily Search Integration:</strong> When receiving a natural language command, Flask triggers Tavily API to search for movement data. Query engineering optimizes search terms, for "jab punch," it searches "boxing jab technique biomechanics step by step." Results are filtered for instructional content, preferring sources like WikiHow, sports coaching sites, and biomechanics research over casual mentions.</p>

            <p><strong>Cohere Agent Orchestration:</strong> The LLM receives search results along with a detailed system prompt defining the robot's kinematic constraints, coordinate system, and safety requirements. Multi turn conversation allows the agent to refine understanding, it can request additional searches if initial results are insufficient or ask clarifying questions about ambiguous motions.</p>

            <p><strong>Motion Guide Schema:</strong> Generated motion guides follow a strict JSON schema defining phase breakdown (preparation, execution, recovery), joint configurations (shoulder, elbow, wrist angles for each arm), timing (duration of each phase, velocity profiles), and safety metadata (maximum angular velocities, collision zones). This structured format enables reliable downstream processing.</p>

            <p><strong>SkillCompiler Implementation:</strong> Python based compiler validates motion guides, applies inverse kinematics transformations, and generates servo command sequences. Uses cubic spline interpolation for smooth trajectories, implements damping for velocity transitions, and enforces mechanical limits (servo angles constrained to 0 to 180 degrees, velocity capped to prevent gear damage).</p>

            <h2>Hardware: ESP32 & Arduino Integration</h2>
            <p>Physical execution involves two microcontrollers working in concert:</p>

            <p><strong>ESP32 (Primary Controller):</strong> Connects to WiFi and receives servo commands via Socket.IO. Controls six servo motors (three per arm: shoulder, elbow, wrist) with PWM signals. Implements real time command queue with microsecond precision timing. Sends position confirmations back to Flask for synchronization feedback.</p>

            <p><strong>Arduino UNO with TFT Display:</strong> Displays an animated monkey avatar reflecting robot operational state. Idle state shows calm monkey, active execution shows excited monkey with moving arms, error states show sad monkey with warning indicators. This playful interface makes the system approachable and provides immediate visual feedback on robot status without requiring screen monitoring.</p>

            <p><strong>Servo Mechanics:</strong> Each robotic arm uses three MG90S micro servos providing 180 degree range of motion. Mechanical linkages amplify servo motion for larger workspace. Arm geometry designed to approximate human arm proportions (upper arm 60% of total length, forearm 40%). 3D printed structural components keep weight low while maintaining rigidity.</p>

            <h1>Key Technical Challenges</h1>

            <h2>Challenge 1: Noisy Unstructured Data</h2>
            <p><strong>Problem:</strong> Web search results for movement descriptions vary wildly in quality, format, and specificity. A search for "jab punch" returns professional boxing coaching (highly detailed), casual forum posts (imprecise), medical biomechanics papers (overly technical), and unrelated content about jabbing buttons or fruit punches.</p>

            <p><strong>Solution:</strong> Implemented multi stage validation in the Cohere agent. First pass filters by relevance score (cosine similarity to query intent). Second pass extracts motion relevant sentences using regex patterns for kinematic language ("extend," "rotate," "angle"). Third pass cross references multiple sources, if three sources agree on "shoulder at 45 degrees," that's validated, if only one mentions it, it's flagged for uncertainty. Final pass performs feasibility checking against robot constraints.</p>

            <p>This pipeline reduced invalid motion guides from 40% to under 5%, dramatically improving reliability.</p>

            <h2>Challenge 2: Timing & Synchronization</h2>
            <p><strong>Problem:</strong> Real time systems demand precise synchronization between visualization, reasoning, and hardware execution. Initial implementation experienced drift where frontend visualization ran ahead of actual servo positions by 2 to 3 seconds, destroying user trust and making the system feel unresponsive.</p>

            <p><strong>Solution:</strong> Implemented timestamp based command protocol with acknowledgment flow. When Flask sends servo commands, each includes a generation timestamp. ESP32 executes commands in order, sending back acknowledgment with actual execution timestamp. Frontend uses this feedback to correct visualization, displaying predicted positions (from command queue) in blue and confirmed positions (from hardware) in green. Latency metrics display shows real time lag.</p>

            <p>Added command batching for high frequency updates, instead of sending 60 individual servo updates per second, batch into 15 updates with 4 servo states each. This reduced Socket.IO overhead and improved synchronization accuracy to within 50ms.</p>

            <h2>Challenge 3: Socket.IO Reliability Under Load</h2>
            <p><strong>Problem:</strong> During rapid motion sequences with many servo updates, Socket.IO connections dropped packets, causing jerky movement and lost commands. Network monitoring revealed 5 to 10% packet loss during peak transmission periods.</p>

            <p><strong>Solution:</strong> Implemented connection health monitoring with automatic reconnection. Client and server exchange heartbeat messages every 500ms. If three consecutive heartbeats fail, connection is considered dead and both sides initiate reconnection. During reconnection, ESP32 halts motion and enters safe state (all servos return to neutral position).</p>

            <p>Switched from JSON text encoding to binary message packs for servo commands, reducing message size from approximately 200 bytes to approximately 40 bytes. This 5x bandwidth reduction eliminated packet loss even during intensive motion sequences.</p>

            <p>Added priority queuing, safety critical commands (emergency stop, position reset) bypass normal queue and transmit immediately with guaranteed delivery confirmation.</p>

            <h2>Challenge 4: LLM Hallucination & Safety</h2>
            <p><strong>Problem:</strong> Language models occasionally hallucinate physically impossible motions or generate servo angles outside mechanical limits. Early testing had the robot attempt 270 degree shoulder rotation (mechanical limit is 180 degrees), causing gear grinding and potential damage.</p>

            <p><strong>Solution:</strong> Implemented multi layer safety validation. SkillCompiler enforces hard constraints: servo angles clamped to 0 to 180 degrees, angular velocities capped at 60 degrees per second, collision zones defined where arms can't overlap. Motion guides that violate constraints are rejected with error feedback to LLM requesting revision.</p>

            <p>Added "safety rehearsal" mode where motions are first validated in simulation (3D visualization) before hardware execution. Users must approve motions after seeing visualization. Emergency stop button instantly halts all servos and clears command queue.</p>

            <p>System logs all generated motion guides and execution outcomes. Failures are flagged for review, and patterns of hallucination are identified for prompt engineering improvements. This feedback loop reduced unsafe motions from 15% to under 2%.</p>

            <h1>Results & Impact</h1>

            <h2>Hackathon Success</h2>
            <p>Built entirely during Hack the North's 36-hour window, the project achieved Top 32 Finalist status among hundreds of submissions. Judges praised the innovative combination of LLMs with robotics, the intuitive user experience, and the technical sophistication of the dual-system architecture.</p>

            <p>Live demonstrations showed the robot learning and executing movements it was never programmed to perform: jab punch, wave gesture, beckoning motion, and celebratory arm raise. Each skill was acquired on the spot through natural language commands, with execution typically within 10 to 15 seconds from command to physical motion.</p>

            <h2>Technical Achievements</h2>
            <ul>
                <li><strong>Zero Shot Learning:</strong> Successfully demonstrated robot skill acquisition without pre training or datasets, relying entirely on real time web search and LLM reasoning</li>
                <li><strong>Sub 15 Second Response:</strong> From natural language input to physical execution typically completed in 10 to 15 seconds (3 to 5s for search, 4 to 7s for LLM reasoning, 2 to 3s for compilation and execution)</li>
                <li><strong>Dual Arm Coordination:</strong> Synchronized bilateral movements with synchronized timing across six servos, achieving fluid motion resembling human gesture</li>
                <li><strong>System Integration:</strong> Successfully combined Next.js frontend, Flask backend, Cohere API, Tavily Search, Socket.IO real time communication, ESP32 hardware control, and Arduino visualization into cohesive working system</li>
                <li><strong>User Experience:</strong> Playful monkey avatar visualization and transparent reasoning display made complex AI and robotics accessible and engaging to non technical audiences</li>
            </ul>

            <h2>Broader Applications</h2>
            <p>The dual-system architecture demonstrates applicability far beyond hackathon demonstrations:</p>

            <p><strong>Educational Robotics:</strong> Students could teach robots physics experiments, chemistry procedures, or engineering tasks using plain language descriptions. This democratizes robotics education by eliminating the need for coding expertise or ML training pipelines. A high school physics class could instruct a robot to demonstrate pendulum motion or lever mechanics by describing the desired behavior.</p>

            <p><strong>Rapid Industrial Prototyping:</strong> Engineers developing automation sequences could prototype workflows using natural language before committing to production code. "Move part from conveyor to inspection station, rotate 90 degrees, scan barcode" becomes executable without writing control logic. This accelerates the design, test, iterate cycle from weeks to hours.</p>

            <p><strong>Assistive Healthcare Robotics:</strong> Therapists could configure rehabilitation robots for patient specific exercises without programming knowledge. "Perform gentle shoulder rotation with 30 degree range of motion" creates a customized therapy routine. As patient mobility improves, therapists adjust exercises through natural language rather than reconfiguring code.</p>

            <p><strong>Research & Exploration:</strong> Scientists could deploy robots in remote or hazardous environments and teach new tasks on the fly as situations evolve. Mars rover encounters unexpected geological formation? Describe the sampling procedure in natural language rather than waiting weeks for a new mission patch.</p>

            <h1>Future Development Directions</h1>

            <h2>Enhanced Perception</h2>
            <p>Current system operates in open loop control, executing pre planned motions without environmental feedback. Adding computer vision would enable closed loop control where the robot adjusts movements based on visual observations. Combining YOLO object detection with current LLM reasoning would allow commands like "pick up the red cup" where the robot searches for the cup visually, plans the grasp, and adapts to object position.</p>

            <h2>Multi Modal Learning</h2>
            <p>Extending beyond text to include video analysis would dramatically improve motion quality. The system could watch YouTube demonstrations of movements, extract skeletal motion data using pose estimation, and convert visual kinematics to servo commands. This combines the precision of visual learning with the flexibility of language based reasoning.</p>

            <h2>Skill Library & Transfer Learning</h2>
            <p>Building a persistent library of learned motions would enable composition and transfer. "Combine the wave gesture with forward step" creates compound movements by sequencing primitives. Skills learned for one task transfer to similar tasks, a grasping motion learned for cups applies to bottles with slight parameter adjustments.</p>

            <h2>Collaborative Multi Robot Systems</h2>
            <p>Extending the architecture to multiple robots would enable collaborative task execution. "Robot A, hold the board steady while Robot B, hammer the nail" requires coordination, role assignment, and synchronized timing. The dual system framework scales naturally, System 2 planning handles task decomposition and role assignment, System 1 execution manages low level synchronization.</p>

            <h1>Conclusion</h1>
            
            <p>Adaptive Learning Robot demonstrates that the future of robotics lies not in massive datasets and months of training, but in intelligent real time adaptation through natural language. By implementing a dual system architecture that mirrors human cognition, deliberate reasoning (System 2) paired with automatic execution (System 1), the project shows that robots can learn skills instantly from unstructured web data.</p>

            <p>The success at Hack the North validates the core insight: the internet already contains the knowledge robots need, we just need better interfaces to transform text into action. Cohere's LLM provides the reasoning capability, Tavily delivers the world's knowledge, and careful engineering bridges the gap from abstract intent to precise servo control.</p>

            <p>This project represents a paradigm shift, from robots as pre programmed machines to robots as adaptive agents that learn and evolve through human like reasoning. The combination of large language models, real time search, and thoughtful system design creates robotics that is accessible, flexible, and aligned with how humans naturally communicate about movement.</p>

            <p>As LLMs grow more capable and multimodal perception improves, this architecture will scale to increasingly sophisticated tasks. The goal is a future where teaching robots is as simple as teaching humans: through description, demonstration, and natural interaction. Adaptive Learning Robot is a step toward that future, where robotics becomes a tool for everyone, not just machine learning experts.</p>
        </section>
    </div>

    <script src="https://cdn.jsdelivr.net/particles.js/2.0.0/particles.min.js"></script>
    <script>
        // Custom cursor
        const cursor = document.createElement('div');
        cursor.classList.add('cursor');
        document.body.appendChild(cursor);

        const cursorGlow = document.createElement('div');
        cursorGlow.classList.add('cursor-glow');
        document.body.appendChild(cursorGlow);

        document.addEventListener('mousemove', (e) => {
            cursor.style.left = e.clientX + 'px';
            cursor.style.top = e.clientY + 'px';
            cursorGlow.style.left = e.clientX + 'px';
            cursorGlow.style.top = e.clientY + 'px';
        });

        document.addEventListener('mousedown', () => {
            cursor.style.transform = 'translate(-50%, -50%) scale(0.8)';
            cursorGlow.style.transform = 'translate(-50%, -50%) scale(0.8)';
        });

        document.addEventListener('mouseup', () => {
            cursor.style.transform = 'translate(-50%, -50%) scale(1)';
            cursorGlow.style.transform = 'translate(-50%, -50%) scale(1)';
        });

        const interactiveElements = document.querySelectorAll('a, button, .stat-card, .tech-item');
        interactiveElements.forEach(element => {
            element.addEventListener('mouseenter', () => {
                cursor.classList.add('hover');
                cursorGlow.classList.add('hover');
            });
            element.addEventListener('mouseleave', () => {
                cursor.classList.remove('hover');
                cursorGlow.classList.remove('hover');
            });
        });

        // Particles.js configuration
        particlesJS('particles-js', {
            particles: {
                number: { value: 70, density: { enable: true, value_area: 800 } },
                color: { value: '#00b4d8' },
                shape: { type: 'circle' },
                opacity: { value: 0.3, random: true },
                size: { value: 3, random: true },
                line_linked: {
                    enable: true,
                    distance: 150,
                    color: '#00b4d8',
                    opacity: 0.2,
                    width: 1
                },
                move: {
                    enable: true,
                    speed: 2,
                    direction: 'none',
                    random: false,
                    straight: false,
                    out_mode: 'out',
                    bounce: false
                }
            },
            interactivity: {
                detect_on: 'canvas',
                events: {
                    onhover: { enable: true, mode: 'grab' },
                    onclick: { enable: true, mode: 'push' },
                    resize: true
                },
                modes: {
                    grab: { distance: 140, line_linked: { opacity: 0.5 } },
                    push: { particles_nb: 4 }
                }
            },
            retina_detect: true
        });

        // Back button scroll behavior
        const backButton = document.querySelector('.back-button');
        let lastScroll = 0;

        // Smooth back button navigation
        backButton.addEventListener('click', (e) => {
            e.preventDefault();
            
            // Create transition overlay
            const overlay = document.createElement('div');
            overlay.style.cssText = `
                position: fixed;
                top: 0;
                left: 0;
                width: 100%;
                height: 100%;
                background: linear-gradient(135deg, #020510 0%, #0a1628 100%);
                z-index: 99999;
                opacity: 0;
                transition: opacity 0.3s ease;
                pointer-events: none;
            `;
            document.body.appendChild(overlay);
            
            // Fade in overlay
            setTimeout(() => {
                overlay.style.opacity = '1';
            }, 10);
            
            // Navigate after overlay is visible
            setTimeout(() => {
                window.location.href = '../index.html';
            }, 200);
        });

        window.addEventListener('scroll', () => {
            const currentScroll = window.pageYOffset;
            
            if (currentScroll > lastScroll && currentScroll > 100) {
                // Scrolling down - hide button
                backButton.classList.add('hidden');
            } else {
                // Scrolling up or at top - show button
                backButton.classList.remove('hidden');
            }
            
            lastScroll = currentScroll;
        });
    </script>
</body>
</html>
