<!DOCTYPE html>
<html lang="en" style="cursor: none !important; background-color: #020510 !important;">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>From Pixels to Precision - Surgical Tool Segmentation AI | Rishi Shah</title>
    <meta name="description" content="Bronze Medal at CWSF - Neural network for real-time surgical tool segmentation in minimally invasive surgery. 95% accuracy, fluorescence-guided surgery support, <0.12s per image.">
    <style>
        /* CRITICAL: Hide cursor and background immediately on page load */
        html, body, * {
            cursor: none !important;
        }
        html, body {
            background-color: #020510 !important;
            margin: 0;
            padding: 0;
        }
    </style>
    <link rel="prefetch" href="../index.html">
    <link rel="preload" href="../style.css" as="style">
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <script src="https://kit.fontawesome.com/17ea408dcc.js" crossorigin="anonymous"></script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        :root {
            --primary: #00b4d8;
            --secondary: #5EEAD4;
            --dark: #020510;
            --darker: #0a1628;
            --light: #e0f2fe;
            --accent: #38bdf8;
        }

        body {
            font-family: 'Poppins', sans-serif;
            background: linear-gradient(135deg, var(--dark) 0%, var(--darker) 100%);
            color: var(--light);
            line-height: 1.6;
            min-height: 100vh;
            overflow-x: hidden;
            cursor: none !important;
        }

        * {
            cursor: none !important;
        }

        #particles-js {
            position: fixed;
            width: 100%;
            height: 100%;
            top: 0;
            left: 0;
            z-index: 5;
            pointer-events: none;
        }

        .back-button {
            position: fixed;
            top: 2rem;
            left: 2rem;
            z-index: 10001;
            width: 48px;
            height: 48px;
            display: flex;
            align-items: center;
            justify-content: center;
            background: rgba(0, 180, 216, 0.1);
            border: 1px solid var(--primary);
            color: var(--primary);
            text-decoration: none;
            border-radius: 50%;
            font-size: 1.2rem;
            transition: all 0.4s cubic-bezier(0.4, 0, 0.2, 1);
            backdrop-filter: blur(10px);
            opacity: 1;
            transform: translateX(0);
        }

        .back-button.hidden {
            opacity: 0;
            transform: translateX(-20px);
            pointer-events: none;
        }

        .back-button:hover {
            background: rgba(0, 180, 216, 0.2);
            transform: scale(1.1);
            box-shadow: 0 0 30px rgba(0, 180, 216, 0.4);
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 6rem 2rem 4rem;
            position: relative;
            z-index: 10;
        }

        .project-header {
            text-align: center;
            margin-bottom: 4rem;
            padding: 3rem 2rem;
            background: rgba(10, 22, 40, 0.5);
            border-radius: 16px;
            border: 1px solid rgba(0, 180, 216, 0.2);
            position: relative;
            overflow: hidden;
        }

        .project-header::before {
            content: '';
            position: absolute;
            top: 0;
            left: -100%;
            width: 100%;
            height: 2px;
            background: linear-gradient(90deg, transparent, var(--primary), transparent);
            animation: scan 3s infinite;
        }

        @keyframes scan {
            0% { left: -100%; }
            100% { left: 100%; }
        }

        .project-header h1 {
            font-size: 3rem;
            font-weight: 700;
            margin-bottom: 1rem;
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        .project-subtitle {
            font-size: 1.2rem;
            color: var(--secondary);
            font-weight: 300;
            margin-bottom: 0.5rem;
        }

        .project-meta {
            display: flex;
            justify-content: center;
            gap: 2rem;
            margin-top: 1.5rem;
            font-size: 0.9rem;
            color: rgba(255, 255, 255, 0.6);
        }

        .section {
            margin-bottom: 2rem;
            padding: 2rem;
            background: rgba(10, 22, 40, 0.3);
            border-radius: 12px;
            border: 1px solid rgba(0, 180, 216, 0.1);
        }

        .section-header {
            font-size: 2rem;
            font-weight: 600;
            margin-bottom: 1.5rem;
            color: var(--primary);
            font-family: 'Courier New', monospace;
        }

        .highlight-box {
            background: rgba(0, 180, 216, 0.05);
            border-left: 4px solid var(--primary);
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-radius: 8px;
        }

        /* Document-style formatting */
        .document-section {
            margin-bottom: 2rem;
            padding: 3rem;
            background: rgba(10, 22, 40, 0.3);
            border-radius: 12px;
            border: 1px solid rgba(0, 180, 216, 0.1);
        }

        .document-section h1 {
            font-size: 2rem;
            font-weight: 600;
            color: var(--primary);
            font-family: 'Courier New', monospace;
            margin-bottom: 1.5rem;
            margin-top: 2.5rem;
        }

        .document-section h1:first-child {
            margin-top: 0;
        }

        .document-section h2 {
            font-size: 1.3rem;
            font-weight: 500;
            color: var(--secondary);
            margin-bottom: 1rem;
            margin-top: 2rem;
        }

        .document-section p {
            font-size: 1rem;
            line-height: 1.8;
            color: rgba(255, 255, 255, 0.85);
            margin-bottom: 1rem;
        }

        .document-section ul {
            margin: 1rem 0 1rem 1.5rem;
            list-style-type: disc;
        }

        .document-section ul li {
            font-size: 1rem;
            line-height: 1.8;
            color: rgba(255, 255, 255, 0.85);
            margin-bottom: 0.5rem;
        }

        .document-section strong {
            color: #ffffff;
            font-weight: 600;
        }

        .video-container {
            position: relative;
            padding-bottom: 56.25%;
            height: 0;
            overflow: hidden;
            margin: 2rem 0;
            border-radius: 12px;
            border: 2px solid rgba(0, 180, 216, 0.3);
        }

        .video-container video {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
        }

        .stats-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 1.5rem;
            margin: 2rem 0;
        }

        .stat-card {
            background: rgba(0, 180, 216, 0.05);
            padding: 1.5rem;
            border-radius: 12px;
            border: 1px solid rgba(0, 180, 216, 0.2);
            text-align: center;
            transition: all 0.3s ease;
        }

        .stat-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 30px rgba(0, 180, 216, 0.2);
            border-color: var(--primary);
        }

        .stat-number {
            font-size: 2.5rem;
            font-weight: 700;
            color: var(--primary);
            margin-bottom: 0.5rem;
        }

        .stat-label {
            font-size: 0.9rem;
            color: rgba(255, 255, 255, 0.7);
        }

        .tech-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 1rem;
            margin: 2rem 0;
        }

        .tech-item {
            background: rgba(0, 180, 216, 0.05);
            padding: 1rem;
            border-radius: 8px;
            border: 1px solid rgba(0, 180, 216, 0.2);
            text-align: center;
            font-weight: 500;
            transition: all 0.3s ease;
        }

        .tech-item:hover {
            background: rgba(0, 180, 216, 0.1);
            border-color: var(--primary);
            transform: scale(1.05);
        }

        .feature-list {
            list-style: none;
            padding: 0;
        }

        .feature-list li {
            padding: 1rem;
            margin: 0.5rem 0;
            background: rgba(0, 180, 216, 0.03);
            border-radius: 8px;
            border-left: 3px solid var(--primary);
            transition: all 0.3s ease;
        }

        .feature-list li:hover {
            background: rgba(0, 180, 216, 0.08);
            transform: translateX(10px);
        }

        .architecture-diagram {
            background: rgba(2, 5, 16, 0.6);
            border: 2px solid rgba(0, 180, 216, 0.3);
            border-radius: 12px;
            padding: 2rem;
            margin: 2rem 0;
            text-align: center;
        }

        .process-flow {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(220px, 1fr));
            gap: 1.4rem;
            margin: 2rem 0;
        }

        .process-step {
            padding: 1.5rem;
            border-radius: 14px;
            background: rgba(0, 180, 216, 0.06);
            border: 1px solid rgba(0, 180, 216, 0.2);
            transition: transform 0.3s ease, border 0.3s ease, box-shadow 0.3s ease;
        }

        .process-step:hover {
            transform: translateY(-6px);
            border-color: var(--primary);
            box-shadow: 0 10px 25px rgba(0, 180, 216, 0.25);
        }

        .step-number {
            background: var(--primary);
            color: var(--dark);
            width: 30px;
            height: 30px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: 700;
            margin-bottom: 0.5rem;
        }

        @media (max-width: 768px) {
            .project-header h1 {
                font-size: 2rem;
            }

            .container {
                padding: 5rem 1rem 2rem;
            }

            .back-button {
                top: 1rem;
                left: 1rem;
                padding: 0.5rem 1rem;
            }

            .stats-grid {
                grid-template-columns: 1fr;
            }

        }

        .project-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            margin-top: 1rem;
            padding: 0.75rem 1.5rem;
            background: rgba(0, 180, 216, 0.1);
            border: 1px solid var(--primary);
            color: var(--primary);
            text-decoration: none;
            border-radius: 8px;
            transition: all 0.3s ease;
        }

        .project-link:hover {
            background: rgba(0, 180, 216, 0.2);
            box-shadow: 0 0 20px rgba(0, 180, 216, 0.3);
            transform: translateY(-2px);
        }

        /* Custom Cursor */
        .cursor {
            width: 16px;
            height: 16px;
            border: 2px solid #00b4d8;
            border-radius: 50%;
            position: fixed;
            pointer-events: none;
            z-index: 9999;
            transition: transform 0.05s ease-out, opacity 0.2s ease, width 0.2s ease, height 0.2s ease;
            transform: translate(-50%, -50%);
            box-shadow: 0 0 10px rgba(0, 180, 220, 0.5);
            opacity: 1;
            will-change: transform;
        }

        .cursor.hover {
            width: 32px;
            height: 32px;
            background: rgba(0, 180, 220, 0.1);
            border-width: 3px;
            box-shadow: 0 0 20px rgba(0, 180, 220, 0.8);
        }

        .cursor-glow {
            width: 40px;
            height: 40px;
            background: radial-gradient(circle, rgba(0, 180, 220, 0.2) 0%, transparent 70%);
            border-radius: 50%;
            position: fixed;
            pointer-events: none;
            z-index: 9998;
            transition: transform 0.1s ease-out, width 0.2s ease, height 0.2s ease;
            transform: translate(-50%, -50%);
            will-change: transform;
        }

        .cursor-glow.hover {
            width: 60px;
            height: 60px;
            background: radial-gradient(circle, rgba(0, 180, 220, 0.3) 0%, transparent 70%);
        }

        a, button, .stat-card, .tech-item, .feature-list li {
            cursor: none !important;
        }
    </style>
</head>
<body>
    <div id="particles-js"></div>
    
    <a href="../index.html" class="back-button">
        <i class="fas fa-arrow-left"></i>
    </a>

    <div class="container">
        <div class="project-header">
            <div class="project-subtitle">CANADA-WIDE SCIENCE FAIR BRONZE MEDALIST</div>
            <h1>From Pixels to Precision</h1>
            <div class="project-subtitle">Neural Network for Real Time Surgical Tool Segmentation</div>
            <div class="project-meta">
                <span>ü•â Bronze Medal - CWSF</span>
                <span>üè• Synaptive Medical Partnership</span>
                <span>‚ö° 95% Accuracy</span>
            </div>
            <a href="https://projectboard.world/ysc/project/from-pixels-to-precision" target="_blank" class="project-link">
                <i class="fas fa-external-link-alt"></i> View on Project Board
            </a>
        </div>

        <!-- Box 2: Overview -->
        <section class="section">
            <h2 class="section-header">Overview</h2>
            
            <div class="video-container">
                <video 
                    id="project-video"
                    controls
                    autoplay
                    muted
                    loop>
                    <source src="../images/tool_segmentation.mp4" type="video/mp4">
                    <source src="../images/tool_segmentation.avi" type="video/x-msvideo">
                    Your browser does not support the video tag.
                </video>
            </div>
            
            <div class="highlight-box">
                <p><strong>From Pixels to Precision</strong> addresses a critical challenge in minimally invasive surgery: nearly 30% of surgical complications are caused by misrecognition of artifacts during operations. This neural network segments surgical tools from their background in real time, providing surgeons with an AI guided assist system that enhances precision and reduces the risk of errors.</p>
            </div>
            <p>As robotic and minimally invasive surgeries become increasingly common, they offer patients reduced pain, shorter hospital stays, and faster recovery times. However, these procedures are prone to errors caused by limited depth perception and visual distortion through surgical cameras. Inspired by how autonomous vehicles use AI for obstacle recognition, this project applies the same technology to revolutionize surgical procedures.</p>
            
            <p style="margin-top: 1rem;">The neural network supports both regular illumination and fluorescence guided surgery, making it ideal for modern surgical practices. With support from <strong>Synaptive Medical Inc.</strong> and <strong>Robarts Research Institute</strong>, the system was trained on real surgical footage captured through a Modus V exoscope.</p>
        </section>

        <section class="section">
            <h2 class="section-header">Technology Stack</h2>
            <div class="tech-grid">
                <div class="tech-item">Python</div>
                <div class="tech-item">Keras</div>
                <div class="tech-item">TensorFlow</div>
                <div class="tech-item">NumPy</div>
                <div class="tech-item">MATLAB</div>
                <div class="tech-item">ITK-SNAP</div>
                <div class="tech-item">U-Net</div>
                <div class="tech-item">OpenCV</div>
            </div>
        </section>

        <!-- Box 4: Main Document Content -->
        <section class="document-section">
            <h1>Motivation & Significance</h1>

            <h2>The Problem</h2>
            <p>Robotic surgeries offer numerous advantages over traditional procedures, including:</p>
            <ul>
                <li>Reduced blood loss and smaller incisions</li>
                <li>Faster recovery rates and shorter hospital stays</li>
                <li>Greater precision through robotic arm stabilization</li>
            </ul>

            <p>However, these benefits come with significant challenges:</p>
            <ul>
                <li><strong>Limited Vision:</strong> Surgeons have restricted view of the surgical site due to camera distortion affecting depth and size perception of 3D structures</li>
                <li><strong>High Error Rate:</strong> According to research on minimally invasive surgery performance errors, nearly 30% of surgical complications are caused by misrecognition during operations</li>
                <li><strong>Risk of Damage:</strong> Misidentification can lead to nerve damage, incorrect surgical maneuvers, or damage to surrounding tissue</li>
            </ul>

            <h2>The Solution</h2>
            <p>Just as autonomous vehicles use AI to predict safe driving lanes based on obstacle recognition, this neural network acts as a guide to support surgeons' experience and skills. By accurately segmenting surgical tools from background tissue, the system:</p>
            <ul>
                <li><strong>Enhances Surgical Precision:</strong> Provides real time visual guidance to reduce misrecognition errors</li>
                <li><strong>Increases Patient Safety:</strong> Minimizes risk of complications caused by visual confusion</li>
                <li><strong>Supports Fluorescence Guided Surgery:</strong> Works with both regular and fluorescent illumination for modern surgical techniques</li>
                <li><strong>Saves Time:</strong> Automated segmentation is dramatically faster than manual approaches, allowing surgeons to focus on critical aspects of procedures</li>
            </ul>

            <h1>Neural Network Architecture</h1>

            <h2>U-Net Implementation</h2>
            <p>The project utilizes <strong>U-Net architecture</strong>, a fully convolutional neural network specifically designed for medical image segmentation. Published in 2015, U-Net revolutionized deep learning for biomedical applications by learning effectively from small training samples.</p>

            <p><strong>Key Components:</strong></p>
            <ul>
                <li><strong>Encoder:</strong> Extracts hierarchical features through convolutional layers, progressively reducing spatial dimensions while increasing feature depth</li>
                <li><strong>Decoder:</strong> Up samples features using transpose convolution, reconstructing spatial resolution for pixel level predictions</li>
                <li><strong>Skip Connections:</strong> Directly links encoder features to decoder, preserving fine grained spatial details lost during down sampling, critical for accurate boundary segmentation</li>
            </ul>

            <h2>Training Pipeline</h2>
            <p>The network was trained using a systematic 4-step process:</p>
            <ul>
                <li><strong>Data Preparation:</strong> 300 images manually segmented using ITK-SNAP software</li>
                <li><strong>Preprocessing:</strong> Fluorescent images converted to grayscale for intensity based learning</li>
                <li><strong>Training:</strong> Keras + Python implementation, approximately 2 hours on GPU</li>
                <li><strong>Validation:</strong> DICE coefficient comparison on test images</li>
            </ul>

            <h2>DICE Coefficient Evaluation</h2>
            <p>Segmentation accuracy was measured using the <strong>DICE coefficient</strong>, a statistical measure of overlap between predicted and ground-truth segmentation masks. The DICE score ranges from 0 (no overlap) to 1 (perfect match), calculated as 2 times the intersection divided by the sum of both mask areas.</p>

            <h1>Methodology</h1>

            <h2>1. Manual Segmentation (Training Data Creation)</h2>
            <p>Training data was captured using a <strong>Modus V exoscope by Synaptive Medical Inc.</strong> during experimental procedures on pig brain tissue, representative of real surgical environments.</p>
            <ul>
                <li><strong>ITK-SNAP Software:</strong> 300 images manually segmented by breaking down into RGB components for grayscale viewing</li>
                <li><strong>Dual Format Storage:</strong> NifTI files (for ITK SNAP compatibility) and PNG files (for visualization)</li>
                <li><strong>Binary Classification:</strong> Pixels labeled as either "surgical tool" or "background tissue"</li>
                <li><strong>Quality Control:</strong> All segmentations double checked for accuracy to maximize ground truth precision</li>
            </ul>

            <h2>2. Fluorescence Challenge Solution</h2>
            <p><strong>Initial Problem:</strong> Neural network struggled with fluorescent blue images due to color variance issues.</p>
            <p><strong>Solution:</strong> Converted fluorescent images to grayscale using MATLAB, enabling the network to distinguish tools based on color intensity rather than color variance. This dramatically improved segmentation accuracy for fluorescence guided surgery scenarios.</p>

            <h2>3. Training Process</h2>
            <ul>
                <li><strong>Framework:</strong> Keras with Python backend</li>
                <li><strong>Training Duration:</strong> Approximately 2 hours on GPU</li>
                <li><strong>Validation:</strong> Same training images re-inputted to compare prediction vs. ground truth segmentations</li>
                <li><strong>Optimization:</strong> Adam optimizer with binary cross entropy loss function</li>
            </ul>

            <h2>4. Testing Protocol</h2>
            <p>To evaluate real-world performance, a completely new surgical video (unseen during training) was tested:</p>
            <ul>
                <li>MATLAB code breaks video into PNG frames</li>
                <li>Neural network segments each frame (less than 0.12s per image)</li>
                <li>Segmented frames compiled back into video</li>
                <li>Side by side analysis of original vs. segmented video</li>
            </ul>

            <p><strong>Test Categories:</strong> Four scenarios tested with 20 random images each:</p>
            <ul>
                <li>Regular illumination (pink) with one surgical tool</li>
                <li>Fluorescent blue with one surgical tool</li>
                <li>Regular illumination (pink) with two surgical tools</li>
                <li>Fluorescent blue with two surgical tools</li>
            </ul>

            <h1>Results & Analysis</h1>

            <h2>Performance Metrics</h2>
            <ul>
                <li><strong>Overall DICE Accuracy:</strong> Approximately 95%</li>
                <li><strong>Processing Speed:</strong> Less than 1 minute to segment 500 images</li>
                <li><strong>Per Image Time:</strong> 0.12s per 600√ó800 pixel image</li>
                <li><strong>Test Scenarios:</strong> 4 different surgical environments</li>
            </ul>

            <h2>Key Findings</h2>
            <p><strong>Exceptional Speed & Efficiency:</strong> The neural network demonstrated remarkable processing speed, segmenting approximately 500 images in under one minute. Each 600√ó800 pixel image required less than 0.12 seconds, which is fast enough for real-time surgical assistance.</p>

            <p><strong>High Accuracy Under Normal Lighting:</strong> The network achieved particularly strong performance identifying surgical tools under regular illumination, with DICE coefficients hovering around 95%.</p>

            <p><strong>Fluorescence Support After Preprocessing:</strong> Once fluorescent blue images were converted to grayscale, the network successfully segmented tools with comparable accuracy to regular illumination scenarios.</p>

            <p><strong>Performance Degradation with Multiple Tools:</strong> As more surgical tools were added to the scene, accuracy became less consistent. However, pink images with two tools showed the smallest accuracy drop across all categories.</p>

            <p><strong>Fluorescent Blue Two-Tool Challenge:</strong> The most difficult scenario was fluorescent blue images containing two tools, which showed the least consistent accuracy, validating the initial hypothesis.</p>

            <h2>Limitations & Analysis</h2>
            <ul>
                <li><strong>Limited Training Diversity:</strong> All 300 training images came from a single video source, restricting the network's ability to generalize to unseen surgical scenarios, instruments, or anatomical regions</li>
                <li><strong>Motion Blur Impact:</strong> The primary accuracy-limiting factor was motion blur affecting the edges between surgical tools and tissue, potentially causing false positives or negatives</li>
                <li><strong>Manual Segmentation Dependency:</strong> Network accuracy directly depends on the precision of manual ground-truth segmentations. Any errors in training data propagate to predictions</li>
                <li><strong>Complex Scenario Challenges:</strong> Performance degradation with multiple tools suggests limitations in handling complex surgical scenarios with tool occlusion or overlapping instruments</li>
            </ul>

            <h1>Opportunities for Improvement</h1>

            <h2>1. Advanced Multi-Layer Architecture</h2>
            <p>Implementing a deeper neural network with additional convolutional layers could detect more intricate patterns and features. This would improve the network's ability to handle motion blur by learning more robust edge detection and boundary refinement strategies.</p>

            <h2>2. Dataset Expansion & Diversification</h2>
            <p>Training on multiple video sources across different surgical procedures (brain, cardiac, abdominal, etc.), instrument types (forceps, scalpels, suction tools, etc.), lighting conditions (various fluorescence wavelengths, intensities), and camera systems would dramatically improve generalization and reduce false positives/negatives in novel scenarios.</p>

            <h2>3. Enhanced Ground Truth Quality</h2>
            <p>Improving the precision of original manual segmentations through multiple expert annotators with consensus voting, higher-resolution source imagery, semi-automated segmentation refinement tools, and validation against surgical tool CAD models.</p>

            <h2>4. Motion Blur Mitigation</h2>
            <p>Potential approaches include temporal consistency constraints using video frame sequences, deblurring preprocessing pipeline before segmentation, training data augmentation with synthetic motion blur, and recurrent neural network components for temporal context.</p>

            <h2>5. Multi-Tool Handling</h2>
            <p>To improve performance with multiple surgical instruments: instance segmentation (identifying each tool separately), attention mechanisms to focus on individual instruments, training specifically on crowded surgical scenes, and post-processing algorithms to separate overlapping tool predictions.</p>

            <h1>Broader Applications</h1>

            <h2>Surgical Tool Segmentation</h2>
            <ul>
                <li><strong>Reduced Misrecognition Errors:</strong> Addresses the 30% complication rate caused by artifact misidentification</li>
                <li><strong>Enhanced Precision:</strong> Real-time visual guidance helps surgeons maintain spatial awareness during minimally invasive procedures</li>
                <li><strong>Time Savings:</strong> Automated segmentation dramatically faster than manual approaches, letting surgeons focus on critical decision-making</li>
                <li><strong>Training & Education:</strong> Annotated surgical videos for medical student training and skill assessment</li>
            </ul>

            <h2>Beyond Surgery: Medical Imaging</h2>
            <p>The neural network framework extends far beyond surgical tool segmentation. <strong>Tumor Detection:</strong> Advanced neural networks can identify abnormalities in medical scans that are difficult for the human eye to detect, such as small tumors in early stages. <strong>Organ & Tissue Segmentation:</strong> Automatic delineation of organs, blood vessels, or tissue types in CT, MRI, or ultrasound imaging for diagnosis and surgical planning. <strong>Disease Diagnosis:</strong> Pattern recognition in radiological images for conditions like pneumonia, fractures, or degenerative diseases. <strong>Quantitative Analysis:</strong> Measuring organ volumes, tracking disease progression, or assessing treatment efficacy through precise segmentation.</p>

            <h2>Industry Partnership Impact</h2>
            <p>Collaboration with <strong>Synaptive Medical Inc.</strong> and <strong>Robarts Research Institute</strong> provided access to professional-grade Modus V exoscope surgical footage, real-world surgical environment data unavailable to typical research projects, validation of clinical relevance and potential commercial applications, and mentorship from medical imaging and neurosurgery experts.</p>

            <h1>Conclusion</h1>
            <p>From Pixels to Precision demonstrates that neural networks have transformative potential for surgical procedures and medical imaging. By achieving approximately 95% accuracy while processing images in under 0.12 seconds, the system proves that real-time AI assistance in surgery is not only possible but practical.</p>
            
            <p>This project addresses a critical healthcare challenge: the 30% complication rate in minimally invasive surgeries caused by misrecognition. By providing surgeons with accurate, real-time tool segmentation (including support for fluorescence-guided surgery), the neural network enhances patient safety, increases surgical precision, and allows medical professionals to focus on the most critical aspects of procedures.</p>

            <p>The partnership with Synaptive Medical Inc. and Robarts Research Institute provided invaluable real-world surgical data and clinical validation. Recognition with a Bronze Medal at the Canada-Wide Science Fair affirms the innovation and potential impact of this work.</p>

            <p>Beyond surgical tool segmentation, this research opens doors to broader medical imaging applications: tumor detection, organ segmentation, disease diagnosis, and quantitative analysis. As neural network architectures become more sophisticated and training datasets expand, the potential to revolutionize healthcare and improve countless lives becomes increasingly tangible.</p>
        </section>
    </div>

    <script src="https://cdn.jsdelivr.net/particles.js/2.0.0/particles.min.js"></script>
    <script>
        // Custom cursor
        const cursor = document.createElement('div');
        cursor.classList.add('cursor');
        document.body.appendChild(cursor);

        const cursorGlow = document.createElement('div');
        cursorGlow.classList.add('cursor-glow');
        document.body.appendChild(cursorGlow);

        document.addEventListener('mousemove', (e) => {
            cursor.style.left = e.clientX + 'px';
            cursor.style.top = e.clientY + 'px';
            cursorGlow.style.left = e.clientX + 'px';
            cursorGlow.style.top = e.clientY + 'px';
        });

        document.addEventListener('mousedown', () => {
            cursor.style.transform = 'translate(-50%, -50%) scale(0.8)';
            cursorGlow.style.transform = 'translate(-50%, -50%) scale(0.8)';
        });

        document.addEventListener('mouseup', () => {
            cursor.style.transform = 'translate(-50%, -50%) scale(1)';
            cursorGlow.style.transform = 'translate(-50%, -50%) scale(1)';
        });

        const interactiveElements = document.querySelectorAll('a, button, .stat-card, .tech-item');
        interactiveElements.forEach(element => {
            element.addEventListener('mouseenter', () => {
                cursor.classList.add('hover');
                cursorGlow.classList.add('hover');
            });
            element.addEventListener('mouseleave', () => {
                cursor.classList.remove('hover');
                cursorGlow.classList.remove('hover');
            });
        });

        // Particles.js configuration
        particlesJS('particles-js', {
            particles: {
                number: { value: 70, density: { enable: true, value_area: 800 } },
                color: { value: '#00b4d8' },
                shape: { type: 'circle' },
                opacity: { value: 0.3, random: true },
                size: { value: 3, random: true },
                line_linked: {
                    enable: true,
                    distance: 150,
                    color: '#00b4d8',
                    opacity: 0.2,
                    width: 1
                },
                move: {
                    enable: true,
                    speed: 2,
                    direction: 'none',
                    random: false,
                    straight: false,
                    out_mode: 'out',
                    bounce: false
                }
            },
            interactivity: {
                detect_on: 'canvas',
                events: {
                    onhover: { enable: true, mode: 'grab' },
                    onclick: { enable: true, mode: 'push' },
                    resize: true
                },
                modes: {
                    grab: { distance: 140, line_linked: { opacity: 0.5 } },
                    push: { particles_nb: 4 }
                }
            },
            retina_detect: true
        });

        // Back button scroll behavior
        const backButton = document.querySelector('.back-button');
        let lastScroll = 0;

        // Smooth back button navigation
        backButton.addEventListener('click', (e) => {
            e.preventDefault();
            
            // Create transition overlay
            const overlay = document.createElement('div');
            overlay.style.cssText = `
                position: fixed;
                top: 0;
                left: 0;
                width: 100%;
                height: 100%;
                background: linear-gradient(135deg, #020510 0%, #0a1628 100%);
                z-index: 99999;
                opacity: 0;
                transition: opacity 0.3s ease;
                pointer-events: none;
            `;
            document.body.appendChild(overlay);
            
            // Fade in overlay
            setTimeout(() => {
                overlay.style.opacity = '1';
            }, 10);
            
            // Navigate after overlay is visible
            setTimeout(() => {
                window.location.href = '../index.html';
            }, 200);
        });

        window.addEventListener('scroll', () => {
            const currentScroll = window.pageYOffset;
            
            if (currentScroll > lastScroll && currentScroll > 100) {
                // Scrolling down - hide button
                backButton.classList.add('hidden');
            } else {
                // Scrolling up or at top - show button
                backButton.classList.remove('hidden');
            }
            
            lastScroll = currentScroll;
        });
    </script>
</body>
</html>
